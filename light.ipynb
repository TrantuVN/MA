{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [09:39<00:00, 19.31s/trial, best loss: 0.007600191993309882]\n",
      "Raw best hyperopt params: {'bagging_fraction': np.float64(0.9711428019901442), 'bagging_freq': np.float64(3.0), 'feature_fraction': np.float64(0.7758107849942487), 'lambda_l1': np.float64(0.0002956434439069393), 'lambda_l2': np.float64(0.008192413045405612), 'learning_rate': np.float64(0.055704635679263516), 'max_bin': np.float64(114.0), 'max_depth': np.int64(2), 'min_data_in_leaf': np.float64(21.0), 'min_gain_to_split': np.float64(0.000415433449785654), 'num_leaves': np.float64(165.0)}\n",
      "Best LightGBM params: {'objective': 'regression', 'metric': 'rmse', 'boosting_type': 'gbdt', 'feature_pre_filter': False, 'verbosity': -1, 'seed': 10, 'learning_rate': np.float64(0.055704635679263516), 'num_leaves': 165, 'max_depth': 12, 'min_data_in_leaf': 21, 'feature_fraction': np.float64(0.7758107849942487), 'bagging_fraction': np.float64(0.9711428019901442), 'bagging_freq': 3, 'lambda_l1': np.float64(0.0002956434439069393), 'lambda_l2': np.float64(0.008192413045405612), 'min_gain_to_split': np.float64(0.000415433449785654), 'max_bin': 114, 'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0}\n",
      "Holdout metrics: RMSE: 85765.534028 | MAE: 19262.991230 | R^2: 0.922099 | best_iter: 192\n"
     ]
    }
   ],
   "source": [
    "# Set seed and max evaluations\n",
    "SEED = 10\n",
    "MAX_EVALS = 30\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('C:/Users/Multiplexon/Desktop/data/2/total_selected_augmented.csv')\n",
    "features = ['Transaction Hash_len', 'Original_len', 'signature_len', 'From_len', 'To_len',\n",
    "            'sender_len', 'paymaster_len', 'Txn Fee', 'logIndex', 'actualGasCost',\n",
    "            'actualGasUsed', 'nonce', 'success', 'Blockno', 'DateTime_ts']\n",
    "X = df[features].astype(float)\n",
    "y = df['Gas Used'].astype(float)\n",
    "\n",
    "# Split 80/20 for tuning\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_X.fit_transform(X_train), columns=X.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(scaler_X.transform(X_val), columns=X.columns, index=X_val.index)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Hyperopt search space for LightGBM\n",
    "space = {\n",
    "    \"reg_params\": {\n",
    "        \"learning_rate\": hp.uniform(\"lgb_lr\", 0.01, 0.2),\n",
    "        \"num_leaves\": scope.int(hp.quniform(\"lgb_nl\", 31, 511, 1)),\n",
    "        \"max_depth\": scope.int(hp.choice(\"lgb_md\", [-1, 2, 5, 10])),\n",
    "        \"min_data_in_leaf\": scope.int(hp.quniform(\"lgb_mdl\", 20, 200, 1)),\n",
    "        \"lambda_l1\": hp.uniform(\"lgb_l1\", 1.0, 10.0),\n",
    "        \"lambda_l2\": hp.uniform(\"lgb_l2\", 1.0, 10.0),\n",
    "        \"min_gain_to_split\": hp.uniform(\"lgb_mgs\", 0.0, 2.0),\n",
    "        \"feature_fraction\": hp.uniform(\"lgb_ff\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": hp.uniform(\"lgb_bf\", 0.6, 1.0),\n",
    "        \"bagging_freq\": scope.int(hp.quniform(\"lgb_bfreq\", 0, 5, 1)),\n",
    "        \"max_bin\": scope.int(hp.quniform(\"lgb_mb\", 63, 255, 1)),\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"seed\": SEED,\n",
    "        \"verbosity\": -1,\n",
    "        \"device\": \"gpu\",\n",
    "        \"gpu_platform_id\": 0,\n",
    "        \"gpu_device_id\": 0\n",
    "    },\n",
    "    \"fit_params\": {}\n",
    "}\n",
    "\n",
    "class HPOptimiser:\n",
    "    def __init__(self, Xtr, Xval, ytr, yval):\n",
    "        self.Xtr, self.Xval, self.ytr, self.yval = Xtr, Xval, ytr, yval\n",
    "\n",
    "    def lgb_reg(self, p):\n",
    "        rp = p[\"reg_params\"].copy()\n",
    "        for k in [\"num_leaves\", \"max_depth\", \"min_data_in_leaf\", \"bagging_freq\", \"max_bin\"]:\n",
    "            rp[k] = int(rp[k])\n",
    "        try:\n",
    "            return self._train_eval(rp)  # GPU\n",
    "        except Exception:\n",
    "            rp[\"device\"] = \"cpu\"  # Fallback CPU\n",
    "            return self._train_eval(rp)\n",
    "\n",
    "    def _train_eval(self, rp):\n",
    "        params = rp.copy()\n",
    "        num_round = 1000\n",
    "        train_ds = lgb.Dataset(self.Xtr, label=self.ytr, feature_name=list(self.Xtr.columns))\n",
    "        valid_ds = lgb.Dataset(self.Xval, label=self.yval, reference=train_ds)\n",
    "        model = lgb.train(\n",
    "            params, train_ds, num_boost_round=num_round,\n",
    "            valid_sets=[valid_ds], valid_names=['val'],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "        )\n",
    "        pred = model.predict(self.Xval, num_iteration=model.best_iteration)\n",
    "        rmse = float(np.sqrt(mean_squared_error(self.yval, pred)))  # RMSE on scaled data\n",
    "        return {\"loss\": rmse, \"status\": STATUS_OK}\n",
    "\n",
    "# Run Hyperopt (scaled)\n",
    "trials = Trials()\n",
    "optim = HPOptimiser(X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled)\n",
    "best_raw = fmin(fn=optim.lgb_reg, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials=trials, rstate=np.random.default_rng(SEED))\n",
    "best = space_eval(space, best_raw)\n",
    "for k in [\"num_leaves\", \"max_depth\", \"min_data_in_leaf\", \"bagging_freq\", \"max_bin\"]:\n",
    "    best[\"reg_params\"][k] = int(best[\"reg_params\"][k])\n",
    "print(\"Best RMSE (scaled):\", trials.best_trial[\"result\"][\"loss\"])\n",
    "print(\"Best params:\", best[\"reg_params\"])\n",
    "\n",
    "# K-fold evaluation with optimized parameters (scaled)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "metrics_per_fold = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_scaled), 1):\n",
    "    X_tr, X_va = X_train_scaled.iloc[tr_idx], X_train_scaled.iloc[va_idx]\n",
    "    y_tr, y_va = y_train_scaled[tr_idx], y_train_scaled[va_idx]\n",
    "    train_ds = lgb.Dataset(X_tr, y_tr, feature_name=list(X_train_scaled.columns))\n",
    "    valid_ds = lgb.Dataset(X_va, y_va, reference=train_ds)\n",
    "    model = lgb.train(\n",
    "        best[\"reg_params\"], train_ds, num_boost_round=1000,\n",
    "        valid_sets=[valid_ds], valid_names=['val'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "    pred = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "    mse = mean_squared_error(y_va, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_va, pred)\n",
    "    r2 = r2_score(y_va, pred)\n",
    "    metrics_per_fold.append({'fold': fold, 'RMSE': rmse, 'MAE': mae, 'R^2': r2, 'MSE': mse})\n",
    "    print(f'Fold {fold} metrics (scaled): RMSE: {rmse:.6f} | MAE: {mae:.6f} | R^2: {r2:.6f} | MSE: {mse:.6f}')\n",
    "\n",
    "# Aggregate and display K-fold metrics\n",
    "metrics_df = pd.DataFrame(metrics_per_fold)\n",
    "print('\\n=== 10-fold CV metrics (scaled) ===')\n",
    "print(metrics_df.round(6))\n",
    "\n",
    "# Train final model (scaled)\n",
    "rp = best[\"reg_params\"].copy()\n",
    "num_round = 1000\n",
    "try:\n",
    "    train_ds = lgb.Dataset(X_train_scaled, label=y_train_scaled, feature_name=list(X_train_scaled.columns))\n",
    "    valid_ds = lgb.Dataset(X_val_scaled, label=y_val_scaled, reference=train_ds)\n",
    "    final = lgb.train(\n",
    "        rp, train_ds, num_boost_round=num_round,\n",
    "        valid_sets=[valid_ds], valid_names=['val'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "    )\n",
    "except Exception:\n",
    "    rp[\"device\"] = \"cpu\"\n",
    "    final = lgb.train(\n",
    "        rp, train_ds, num_boost_round=num_round,\n",
    "        valid_sets=[valid_ds], valid_names=['val'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "    )\n",
    "\n",
    "pred = final.predict(X_val_scaled, num_iteration=final.best_iteration)\n",
    "val_mse = mean_squared_error(y_val_scaled, pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mae = mean_absolute_error(y_val_scaled, pred)\n",
    "val_r2 = r2_score(y_val_scaled, pred)\n",
    "print(f\"\\nFinal metrics (scaled): RMSE: {val_rmse:.6f} | MAE: {val_mae:.6f} | R^2: {val_r2:.6f} | MSE: {val_mse:.6f} |\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
