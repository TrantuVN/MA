{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:11<00:35,  1.09trial/s, best loss: 0.011410109905839143]"
     ]
    }
   ],
   "source": [
    "SEED = 10\n",
    "MAX_EVALS = 50\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def cast_ints(p):\n",
    "    p = p.copy()\n",
    "    for k in [\"num_leaves\", \"max_depth\", \"min_data_in_leaf\", \"bagging_freq\", \"max_bin\"]:\n",
    "        if k in p:\n",
    "            p[k] = int(p[k])\n",
    "    return p\n",
    "\n",
    "def gpu_flag():\n",
    "    \"\"\"Return correct GPU param for LightGBM version (GPU-ONLY).\"\"\"\n",
    "    try:\n",
    "        major = int(lgb.__version__.split('.')[0])\n",
    "    except Exception:\n",
    "        major = 4\n",
    "    return {\"device\": \"gpu\"} if major >= 4 else {\"device_type\": \"gpu\"}\n",
    "\n",
    "def map_lgbm_params(p):\n",
    "    \"\"\"Map hyperopt keys to LGBMRegressor kwargs.\"\"\"\n",
    "    p = p.copy()\n",
    "    # map regularization names\n",
    "    if \"lambda_l1\" in p:\n",
    "        p[\"reg_alpha\"] = p.pop(\"lambda_l1\")\n",
    "    if \"lambda_l2\" in p:\n",
    "        p[\"reg_lambda\"] = p.pop(\"lambda_l2\")\n",
    "    return p\n",
    "\n",
    "# ---------- 1) Load data ----------\n",
    "df = pd.read_csv('C:/Users/Multiplexon/Desktop/data/2/total_selected_augmented.csv', sep=',')\n",
    "\n",
    "features = ['Transaction Hash_len','Original_len','signature_len',\n",
    "            'From_len','To_len','sender_len','paymaster_len',\n",
    "            'Txn Fee','logIndex','actualGasCost',\n",
    "            'actualGasUsed','nonce','success','Blockno','DateTime_ts']\n",
    "\n",
    "X = df.loc[:, features].astype(float)\n",
    "y = df['Gas Used'].astype(float)\n",
    "\n",
    "# ---------- 2) Split 80/20 for tuning ----------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "# ---------- 3) MinMaxScaler (fit on train for tuning stage) ----------\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_s = pd.DataFrame(scaler_X.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_val_s   = pd.DataFrame(scaler_X.transform(X_val),     columns=X_val.columns,     index=X_val.index)\n",
    "y_train_s = scaler_y.fit_transform(y_train.values.reshape(-1,1)).ravel()\n",
    "y_val_s   = scaler_y.transform(y_val.values.reshape(-1,1)).ravel()\n",
    "\n",
    "# ---------- 4) Hyperopt search space (GPU-safe max_bin ≤ 255) ----------\n",
    "space = {\n",
    "    \"learning_rate\":     hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.2)),\n",
    "    \"num_leaves\":        hp.quniform(\"num_leaves\", 15, 255, 1),\n",
    "    \"max_depth\":         hp.quniform(\"max_depth\", -1, 16, 1),  # -1 = no limit\n",
    "    \"min_data_in_leaf\":  hp.quniform(\"min_data_in_leaf\", 10, 500, 1),\n",
    "    \"feature_fraction\":  hp.uniform(\"feature_fraction\", 0.6, 1.0),\n",
    "    \"bagging_fraction\":  hp.uniform(\"bagging_fraction\", 0.6, 1.0),\n",
    "    \"bagging_freq\":      hp.quniform(\"bagging_freq\", 0, 10, 1),\n",
    "    \"lambda_l1\":         hp.loguniform(\"lambda_l1\", np.log(1e-4), np.log(10.0)),\n",
    "    \"lambda_l2\":         hp.loguniform(\"lambda_l2\", np.log(1e-4), np.log(10.0)),\n",
    "    \"min_split_gain\":    hp.uniform(\"min_split_gain\", 0.0, 1.0),\n",
    "    \"extra_trees\":       hp.choice(\"extra_trees\", [False, True]),\n",
    "    \"max_bin\":           hp.quniform(\"max_bin\", 63, 255, 1),   # GPU limit\n",
    "}\n",
    "\n",
    "# ---------- 5) Objective (GPU-only, scaled RMSE) ----------\n",
    "def objective(params):\n",
    "    params = cast_ints(params)\n",
    "    params[\"max_bin\"] = min(255, params.get(\"max_bin\", 255))\n",
    "    params = map_lgbm_params(params)\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=10000,\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        random_state=SEED,\n",
    "        # GPU-ONLY flag (v3/v4 compatible)\n",
    "        **gpu_flag(),\n",
    "        # forward tuned params:\n",
    "        **params\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_s, y_train_s,\n",
    "        eval_set=[(X_val_s, y_val_s)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=0)  # silence\n",
    "        ]\n",
    "    )\n",
    "    pred_val_s = model.predict(X_val_s)\n",
    "    return {\"loss\": rmse(y_val_s, pred_val_s), \"status\": STATUS_OK,\n",
    "            \"best_iteration\": getattr(model, \"best_iteration_\", None)}\n",
    "\n",
    "# ---------- 6) Run Hyperopt ----------\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective, space=space, algo=tpe.suggest,\n",
    "    max_evals=MAX_EVALS, trials=trials, rstate=np.random.default_rng(SEED)\n",
    ")\n",
    "best = cast_ints(best)\n",
    "best[\"max_bin\"] = min(255, best.get(\"max_bin\", 255))  # (GPU-safe)\n",
    "if \"extra_trees\" in best:\n",
    "    best[\"extra_trees\"] = bool(best[\"extra_trees\"])   # <<< FIX 0/1 -> bool\n",
    "best = map_lgbm_params(best)  # nếu bạn dùng bản LGBMRegressor có map reg_alpha/reg_lambda\n",
    "print(\"Best hyperparams (scaled-RMSE objective):\", best)\n",
    "\n",
    "# ---------- 7) Final 10-fold CV (GPU-only, scaled-only) ----------\n",
    "base_final = dict(\n",
    "    n_estimators=10000,\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"regression\",\n",
    "    random_state=SEED,\n",
    "    **gpu_flag(),\n",
    "    **best\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "fold_rows = [] \n",
    "rmse_s_list, mae_s_list, r2_s_list = [], [], []\n",
    "best_iters = []\n",
    "feat_gain = np.zeros(X.shape[1], dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X), 1):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    # --- per-fold scalers (no leakage)\n",
    "    scX = MinMaxScaler(); scY = MinMaxScaler()\n",
    "    X_tr_s = pd.DataFrame(scX.fit_transform(X_tr), columns=X.columns, index=X_tr.index)\n",
    "    X_va_s = pd.DataFrame(scX.transform(X_va),    columns=X.columns, index=X_va.index)\n",
    "    y_tr_s = scY.fit_transform(y_tr.values.reshape(-1,1)).ravel()\n",
    "    y_va_s = scY.transform(y_va.values.reshape(-1,1)).ravel()\n",
    "\n",
    "    model = LGBMRegressor(**base_final)\n",
    "    model.fit(\n",
    "        X_tr_s, y_tr_s,\n",
    "        eval_set=[(X_va_s, y_va_s)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    bi = getattr(model, \"best_iteration_\", base_final[\"n_estimators\"])\n",
    "    best_iters.append(bi)\n",
    "    pred_va_s = model.predict(X_va_s)\n",
    "    \n",
    "    \n",
    "    mse_val  = float(mean_squared_error(y_va_s, pred_va_s))\n",
    "    rmse_val = float(np.sqrt(mse_val))\n",
    "    mae_val  = float(mean_absolute_error(y_va_s, pred_va_s))\n",
    "    r2_val   = float(r2_score(y_va_s, pred_va_s))\n",
    "\n",
    "    rmse_s_list.append(rmse_s); mae_s_list.append(mae_s); r2_s_list.append(r2_s)\n",
    "    print(f\"{ {'fold': fold, 'MSE': round(mse_val, 10), 'RMSE': round(rmse_val, 8), 'MAE': round(mae_val, 8), 'R^2': round(r2_val, 6)} }\")\n",
    "\n",
    "    # -- lưu vào bảng\n",
    "    fold_rows.append(OrderedDict([\n",
    "        (\"fold\", fold),\n",
    "        (\"MSE\",  mse_val),\n",
    "        (\"RMSE\", rmse_val),\n",
    "        (\"MAE\",  mae_val),\n",
    "        (\"R^2\",  r2_val),\n",
    "    ]))\n",
    "    df_folds = pd.DataFrame(fold_rows)\n",
    "# làm tròn khi hiển thị\n",
    "print(\"\\nKết quả từng fold (scaled):\")\n",
    "print(df_folds.round({\"MSE\": 10, \"RMSE\": 8, \"MAE\": 8, \"R^2\": 6}).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
