{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostRegressor\n",
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction Hash_len</th>\n",
       "      <th>Original_len</th>\n",
       "      <th>signature_len</th>\n",
       "      <th>From_len</th>\n",
       "      <th>To_len</th>\n",
       "      <th>sender_len</th>\n",
       "      <th>paymaster_len</th>\n",
       "      <th>Txn Fee</th>\n",
       "      <th>Gas Used</th>\n",
       "      <th>logIndex</th>\n",
       "      <th>actualGasCost</th>\n",
       "      <th>actualGasUsed</th>\n",
       "      <th>nonce</th>\n",
       "      <th>success</th>\n",
       "      <th>Blockno</th>\n",
       "      <th>DateTime_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>1412</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>360963</td>\n",
       "      <td>77</td>\n",
       "      <td>5.580000e+15</td>\n",
       "      <td>432020</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18909051</td>\n",
       "      <td>1704069120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>97441</td>\n",
       "      <td>80</td>\n",
       "      <td>1.250000e+15</td>\n",
       "      <td>110855</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18909074</td>\n",
       "      <td>1704069360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>1636</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>306649</td>\n",
       "      <td>60</td>\n",
       "      <td>4.450000e+15</td>\n",
       "      <td>374766</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18909211</td>\n",
       "      <td>1704071040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>1636</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>281030</td>\n",
       "      <td>55</td>\n",
       "      <td>4.270000e+15</td>\n",
       "      <td>349159</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18909215</td>\n",
       "      <td>1704071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1636</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>142131</td>\n",
       "      <td>155</td>\n",
       "      <td>2.110000e+15</td>\n",
       "      <td>164548</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18909225</td>\n",
       "      <td>1704071220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction Hash_len  Original_len  signature_len  From_len  To_len  \\\n",
       "0                    32          1412              4        20      20   \n",
       "1                    32           900              4        20      20   \n",
       "2                    32          1636              4        20      20   \n",
       "3                    32          1636              4        20      20   \n",
       "4                    32          1636              4        20      20   \n",
       "\n",
       "   sender_len  paymaster_len   Txn Fee  Gas Used  logIndex  actualGasCost  \\\n",
       "0          20             20  0.004663    360963        77   5.580000e+15   \n",
       "1          20             20  0.001098     97441        80   1.250000e+15   \n",
       "2          20             20  0.003643    306649        60   4.450000e+15   \n",
       "3          20             20  0.003438    281030        55   4.270000e+15   \n",
       "4          20             20  0.001824    142131       155   2.110000e+15   \n",
       "\n",
       "   actualGasUsed  nonce  success   Blockno  DateTime_ts  \n",
       "0         432020    6.0        1  18909051   1704069120  \n",
       "1         110855   33.0        1  18909074   1704069360  \n",
       "2         374766    7.0        1  18909211   1704071040  \n",
       "3         349159    8.0        1  18909215   1704071100  \n",
       "4         164548    9.0        1  18909225   1704071220  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/Multiplexon/Desktop/data/d6/total 06.csv', sep=',')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== SCATTER (MinMax[train]) ==================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# y_true/y_pred trn thang MinMax ( c t phn nh gi cui)\n",
    "y_true_mm = y_test_mm\n",
    "y_pred_mm = y_pred_test_mm\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.scatter(y_true_mm, y_pred_mm, s=40, alpha=0.8, color=\"lightskyblue\", edgecolors=\"k\")\n",
    "\n",
    "# V ng y = x theo gii hn hin ti ca trc\n",
    "x0, x1 = ax.get_xlim()\n",
    "y0, y1 = ax.get_ylim()\n",
    "lo, hi = min(x0, y0), max(x1, y1)\n",
    "ax.plot([lo, hi], [lo, hi], \"--\", linewidth=1,color=\"red\")\n",
    "\n",
    "ax.set_title(\"CatBoost+OT: Predictions vs Actual (MinMax[train])\")\n",
    "ax.set_xlabel(\"Actual (MinMax)\")\n",
    "ax.set_ylabel(\"Predicted (MinMax)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      " BEST PARAMETERS: {'stage': 600, 'rounds': 5, 'lam': 0.07, 'k_last': 2}\n",
      " Final R on held-out set: 0.855267\n"
     ]
    }
   ],
   "source": [
    "#optimize parameters OT-catboost with RandomizedSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ================== HELPER FUNCTIONS ==================\n",
    "def combine_last_k_leaf_ids(leaf_mat, k_last=1):\n",
    "    L = np.asarray(leaf_mat)\n",
    "    if L.ndim == 1:\n",
    "        return L.astype(np.int64)\n",
    "    k = max(1, min(int(k_last), L.shape[1]))\n",
    "    sub = L[:, -k:]\n",
    "    base = int(sub.max()) + 1\n",
    "    powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "    return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    penalties = np.zeros(len(unique_leaves), dtype=np.float64)\n",
    "    global_residuals = residuals\n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        if len(leaf_residuals) < 8:\n",
    "            continue\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        penalties[i] = lam * w_dist\n",
    "    return penalties\n",
    "\n",
    "# ================== CUSTOM ESTIMATOR ==================\n",
    "class OTCatBoostRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, lam=0.05, k_last=2, stage=400, rounds=5,\n",
    "                 depth=8, learning_rate=0.05, l2_leaf_reg=5):\n",
    "        self.lam = lam\n",
    "        self.k_last = k_last\n",
    "        self.stage = stage\n",
    "        self.rounds = rounds\n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_leaf_reg = l2_leaf_reg\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        w = np.ones(len(y), dtype=np.float64)\n",
    "        model = None\n",
    "        for r in range(1, self.rounds + 1):\n",
    "            iters = (model.tree_count_ if model else 0) + self.stage\n",
    "            booster = CatBoostRegressor(\n",
    "                iterations=iters,\n",
    "                depth=self.depth,\n",
    "                learning_rate=self.learning_rate,\n",
    "                l2_leaf_reg=self.l2_leaf_reg,\n",
    "                loss_function=\"RMSE\",\n",
    "                random_seed=SEED,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False,\n",
    "                thread_count=-1\n",
    "            )\n",
    "            booster.fit(X, y, sample_weight=w, init_model=None if model is None else model)\n",
    "            model = booster\n",
    "            res = y - model.predict(X)\n",
    "            leaf_idx = model.calc_leaf_indexes(X,\n",
    "                ntree_start=max(model.tree_count_ - self.k_last, 0),\n",
    "                ntree_end=model.tree_count_)\n",
    "            leaf_idx = np.asarray(leaf_idx)\n",
    "            if leaf_idx.ndim == 1:\n",
    "                leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "            codes = combine_last_k_leaf_ids(leaf_idx, self.k_last)\n",
    "            _, inv = np.unique(codes, return_inverse=True)\n",
    "            pen = real_ot_penalty(res, inv, self.lam)\n",
    "            w *= np.exp(-pen[inv])\n",
    "            w /= (w.mean() + 1e-12)\n",
    "            w = np.clip(w, 0.05, 10.0)\n",
    "        self.model_ = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "# ================== LOAD AND PREPROCESS ==================\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d6/total 06.csv\"\n",
    "FEATS = ['Original_len','Txn Fee','logIndex','actualGasCost','Blockno','DateTime_ts','nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "df = pd.read_csv(PATH, usecols=lambda c: c in FEATS + [TARGET])\n",
    "for c in FEATS + [TARGET]:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "df.dropna(subset=[TARGET], inplace=True)\n",
    "\n",
    "X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "y = df[TARGET].to_numpy(dtype=np.float64)\n",
    "\n",
    "X_train, X_val, y_train_raw, y_val_raw = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Impute + scale\n",
    "imp = SimpleImputer(strategy=\"median\").fit(X_train)\n",
    "X_train_imp = imp.transform(X_train)\n",
    "X_val_imp = imp.transform(X_val)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_imp)\n",
    "X_train_scaled = scaler.transform(X_train_imp).astype(np.float32)\n",
    "X_val_scaled = scaler.transform(X_val_imp).astype(np.float32)\n",
    "\n",
    "# Log transform\n",
    "y_train_log = np.log1p(y_train_raw.astype(np.float64))\n",
    "y_val_log = np.log1p(y_val_raw.astype(np.float64))\n",
    "\n",
    "# ================== RANDOM SEARCH CV ==================\n",
    "param_dist = {\n",
    "    'lam': [0.03, 0.05, 0.07],\n",
    "    'k_last': [2, 3, 4],\n",
    "    'stage': [400, 600, 800],\n",
    "    'rounds': [3, 5, 7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=OTCatBoostRegressor(),\n",
    "    param_distributions=param_dist,\n",
    "    scoring=make_scorer(r2_score),\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train_scaled, y_train_log)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# ================== EVALUATION ==================\n",
    "y_pred_log = best_model.predict(X_val_scaled)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_val_log)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\n BEST PARAMETERS:\", search.best_params_)\n",
    "print(f\" Final R on held-out set: {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "Fold 1 [MinMax]: RMSE=0.004744, MSE=0.000023, MAE=0.002132, R2=0.848444\n",
      "\n",
      "===== Fold 2 =====\n",
      "Fold 2 [MinMax]: RMSE=0.004318, MSE=0.000019, MAE=0.002137, R2=0.867466\n",
      "\n",
      "===== Fold 3 =====\n",
      "Fold 3 [MinMax]: RMSE=0.009863, MSE=0.000097, MAE=0.003276, R2=0.747955\n",
      "\n",
      "===== Fold 4 =====\n",
      "Fold 4 [MinMax]: RMSE=0.005348, MSE=0.000029, MAE=0.002165, R2=0.816161\n",
      "\n",
      "===== Fold 5 =====\n",
      "Fold 5 [MinMax]: RMSE=0.005505, MSE=0.000030, MAE=0.002162, R2=0.808559\n",
      "\n",
      "===== Fold 6 =====\n",
      "Fold 6 [MinMax]: RMSE=0.005129, MSE=0.000026, MAE=0.002167, R2=0.823902\n",
      "\n",
      "===== Fold 7 =====\n",
      "Fold 7 [MinMax]: RMSE=0.003978, MSE=0.000016, MAE=0.002129, R2=0.885429\n",
      "\n",
      "===== Fold 8 =====\n",
      "Fold 8 [MinMax]: RMSE=0.005337, MSE=0.000028, MAE=0.002166, R2=0.819056\n",
      "\n",
      "===== Fold 9 =====\n",
      "Fold 9 [MinMax]: RMSE=0.005312, MSE=0.000028, MAE=0.002188, R2=0.816413\n",
      "\n",
      "===== Fold 10 =====\n",
      "Fold 10 [MinMax]: RMSE=0.005365, MSE=0.000029, MAE=0.002149, R2=0.824885\n",
      "\n",
      "=== 10-FOLD AVERAGE (MinMax) ===\n",
      "RMSE: 0.005490\n",
      "MSE : 0.000032\n",
      "MAE : 0.002267\n",
      "R  : 0.825827\n"
     ]
    }
   ],
   "source": [
    "#WASSSENTERIN 1/ optimized\n",
    "# ================================================================\n",
    "#  REAL OT-CATBOOST (WASSERSTEIN-1) FOR ENTRYPOINT ADDRESS v06\n",
    "# ================================================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d6/total 06.csv\"\n",
    "FEATS = ['Original_len', 'Txn Fee', 'logIndex', 'actualGasCost', 'Blockno', 'DateTime_ts', 'nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "# OT-CatBoost hyperparameters\n",
    "ROUNDS      = 5\n",
    "STAGE       = 600\n",
    "K_LAST      = 2\n",
    "LAM         = 0.07\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# 1 DATA LOADING & PREPROCESSING\n",
    "# ===================================================\n",
    "use_cols = FEATS + [TARGET]\n",
    "df = pd.read_csv(PATH, usecols=lambda c: c in use_cols)\n",
    "\n",
    "for c in use_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "y_raw = df[TARGET].to_numpy(dtype=np.float64)\n",
    "\n",
    "# ===================================================\n",
    "# 2 HELPER FUNCTIONS\n",
    "# ===================================================\n",
    "def combine_last_k_leaf_ids(leaf_mat, k_last=1):\n",
    "    \"\"\"Combine last k leaf indexes into unique group IDs.\"\"\"\n",
    "    L = np.asarray(leaf_mat)\n",
    "    if L.ndim == 1:\n",
    "        return L.astype(np.int64)\n",
    "    k = max(1, min(int(k_last), L.shape[1]))\n",
    "    sub = L[:, -k:]\n",
    "    base = int(sub.max()) + 1\n",
    "    powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "    return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    \"\"\"Compute true Wasserstein-1 (Earth Mover's) distance.\"\"\"\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein-1 penalties per leaf group.\n",
    "    Penalize groups whose residual distributions deviate \n",
    "    significantly from the global residual distribution.\n",
    "    \"\"\"\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    n_leaves = len(unique_leaves)\n",
    "    penalties = np.zeros(n_leaves, dtype=np.float64)\n",
    "    global_residuals = residuals\n",
    "\n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        if len(leaf_residuals) < 8:\n",
    "            penalties[i] = 0.0\n",
    "            continue\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        penalties[i] = lam * w_dist\n",
    "    return penalties\n",
    "\n",
    "# ===================================================\n",
    "# 3 10-FOLD CROSS VALIDATION\n",
    "# ===================================================\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "metrics_all = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train_raw, y_test_raw = y_raw[train_idx], y_raw[test_idx]\n",
    "\n",
    "    # Target scaling\n",
    "    y_mm_scaler = MinMaxScaler()\n",
    "    y_train_mm = y_mm_scaler.fit_transform(y_train_raw.reshape(-1, 1)).ravel()\n",
    "    y_test_mm  = y_mm_scaler.transform(y_test_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Feature impute + scale\n",
    "    X_train[~np.isfinite(X_train)] = np.nan\n",
    "    X_test[~np.isfinite(X_test)]   = np.nan\n",
    "    imp = SimpleImputer(strategy=\"median\").fit(X_train)\n",
    "    X_train_imp = imp.transform(X_train)\n",
    "    X_test_imp  = imp.transform(X_test)\n",
    "    scaler = MinMaxScaler().fit(X_train_imp)\n",
    "    X_train_s = scaler.transform(X_train_imp).astype(np.float32)\n",
    "    X_test_s  = scaler.transform(X_test_imp).astype(np.float32)\n",
    "\n",
    "    # Target transform (log1p)\n",
    "    y_train = np.log1p(y_train_raw.astype(np.float64))\n",
    "    y_test  = np.log1p(y_test_raw.astype(np.float64))\n",
    "\n",
    "    # ===================================================\n",
    "    # 4 TRAINING LOOP WITH REAL OT REWEIGHTING\n",
    "    # ===================================================\n",
    "    w = np.ones(len(y_train), dtype=np.float64)\n",
    "    model = None\n",
    "\n",
    "    for r in range(1, ROUNDS + 1):\n",
    "        iters = (model.tree_count_ if model is not None else 0) + STAGE\n",
    "        booster = CatBoostRegressor(\n",
    "            iterations=iters, depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "            loss_function=\"RMSE\", random_seed=SEED, verbose=False,\n",
    "            allow_writing_files=False, thread_count=-1\n",
    "        )\n",
    "        booster.fit(\n",
    "            X_train_s, y_train, sample_weight=w,\n",
    "            init_model=None if model is None else model\n",
    "        )\n",
    "        model = booster\n",
    "\n",
    "        # Compute residuals\n",
    "        res = y_train - model.predict(X_train_s)\n",
    "\n",
    "        # Group residuals by leaf path\n",
    "        leaf_idx = model.calc_leaf_indexes(\n",
    "            X_train_s,\n",
    "            ntree_start=max(model.tree_count_ - K_LAST, 0),\n",
    "            ntree_end=model.tree_count_,\n",
    "            thread_count=-1\n",
    "        )\n",
    "        leaf_idx = np.asarray(leaf_idx)\n",
    "        if leaf_idx.ndim == 1:\n",
    "            leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "        codes = combine_last_k_leaf_ids(leaf_idx, k_last=K_LAST)\n",
    "        _, inv = np.unique(codes, return_inverse=True)\n",
    "\n",
    "        # Apply OT penalty (W1 distance)\n",
    "        pen = real_ot_penalty(residuals=res, leaf_assignments=inv, lam=LAM)\n",
    "\n",
    "        # Update weights\n",
    "        w *= np.exp(-pen[inv])\n",
    "        w /= (w.mean() + 1e-12)\n",
    "        w = np.clip(w, 1e-3, 50.0)\n",
    "\n",
    "    # ===================================================\n",
    "    # 5 EVALUATION ON TEST SET\n",
    "    # ===================================================\n",
    "    y_pred_raw = np.expm1(model.predict(X_test_s))\n",
    "    y_pred_mm  = y_mm_scaler.transform(y_pred_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "    mse  = mean_squared_error(y_test_mm, y_pred_mm)\n",
    "    rmse = mean_squared_error(y_test_mm, y_pred_mm, squared=False)\n",
    "    mae  = mean_absolute_error(y_test_mm, y_pred_mm)\n",
    "    r2   = r2_score(y_test_mm, y_pred_mm)\n",
    "\n",
    "    print(f\"Fold {fold} [MinMax]: RMSE={rmse:.6f}, MSE={mse:.6f}, MAE={mae:.6f}, R2={r2:.6f}\")\n",
    "    metrics_all.append([rmse, mse, mae, r2])\n",
    "\n",
    "# ===================================================\n",
    "# 6 SUMMARY OF RESULTS\n",
    "# ===================================================\n",
    "metrics_all = np.array(metrics_all)\n",
    "print(\"\\n=== 10-FOLD AVERAGE (MinMax) ===\")\n",
    "print(f\"RMSE: {metrics_all[:,0].mean():.6f}\")\n",
    "print(f\"MSE : {metrics_all[:,1].mean():.6f}\")\n",
    "print(f\"MAE : {metrics_all[:,2].mean():.6f}\")\n",
    "print(f\"R  : {metrics_all[:,3].mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Fold 1: RMSE: 0.004996, R2: 0.835103\n",
      "Epoch 1, Fold 2: RMSE: 0.005055, R2: 0.823394\n",
      "Epoch 1, Fold 3: RMSE: 0.004530, R2: 0.862155\n",
      "Epoch 1, Fold 4: RMSE: 0.008898, R2: 0.791777\n",
      "Epoch 1, Fold 5: RMSE: 0.005256, R2: 0.825206\n",
      "Epoch 1, Fold 6: RMSE: 0.005319, R2: 0.827685\n",
      "Epoch 1, Fold 7: RMSE: 0.005637, R2: 0.798816\n",
      "Epoch 1, Fold 8: RMSE: 0.005306, R2: 0.811154\n",
      "Epoch 1, Fold 9: RMSE: 0.004819, R2: 0.838474\n",
      "Epoch 1, Fold 10: RMSE: 0.005361, R2: 0.809223\n",
      "Epoch 1 Average - RMSE: 0.005518, MSE: 0.000032, MAE: 0.002264, R2: 0.822299\n",
      "Epoch 2, Fold 1: RMSE: 0.005044, R2: 0.830349\n",
      "Epoch 2, Fold 2: RMSE: 0.005129, R2: 0.825847\n",
      "Epoch 2, Fold 3: RMSE: 0.005233, R2: 0.823560\n",
      "Epoch 2, Fold 4: RMSE: 0.005027, R2: 0.836246\n",
      "Epoch 2, Fold 5: RMSE: 0.008998, R2: 0.786773\n",
      "Epoch 2, Fold 6: RMSE: 0.004963, R2: 0.829984\n",
      "Epoch 2, Fold 7: RMSE: 0.006416, R2: 0.758358\n",
      "Epoch 2, Fold 8: RMSE: 0.004274, R2: 0.868351\n",
      "Epoch 2, Fold 9: RMSE: 0.005338, R2: 0.821236\n",
      "Epoch 2, Fold 10: RMSE: 0.004303, R2: 0.872232\n",
      "Epoch 2 Average - RMSE: 0.005472, MSE: 0.000032, MAE: 0.002263, R2: 0.825294\n",
      "Epoch 3, Fold 1: RMSE: 0.004854, R2: 0.838584\n",
      "Epoch 3, Fold 2: RMSE: 0.004463, R2: 0.859948\n",
      "Epoch 3, Fold 3: RMSE: 0.008638, R2: 0.806356\n",
      "Epoch 3, Fold 4: RMSE: 0.005668, R2: 0.797649\n",
      "Epoch 3, Fold 5: RMSE: 0.006006, R2: 0.778272\n",
      "Epoch 3, Fold 6: RMSE: 0.006014, R2: 0.781320\n",
      "Epoch 3, Fold 7: RMSE: 0.004243, R2: 0.873632\n",
      "Epoch 3, Fold 8: RMSE: 0.004513, R2: 0.860168\n",
      "Epoch 3, Fold 9: RMSE: 0.004701, R2: 0.847599\n",
      "Epoch 3, Fold 10: RMSE: 0.005166, R2: 0.831466\n",
      "Epoch 3 Average - RMSE: 0.005426, MSE: 0.000031, MAE: 0.002263, R2: 0.827500\n",
      "Epoch 4, Fold 1: RMSE: 0.004590, R2: 0.849548\n",
      "Epoch 4, Fold 2: RMSE: 0.004345, R2: 0.869794\n",
      "Epoch 4, Fold 3: RMSE: 0.005860, R2: 0.782085\n",
      "Epoch 4, Fold 4: RMSE: 0.004506, R2: 0.862227\n",
      "Epoch 4, Fold 5: RMSE: 0.010847, R2: 0.721362\n",
      "Epoch 4, Fold 6: RMSE: 0.005171, R2: 0.823244\n",
      "Epoch 4, Fold 7: RMSE: 0.005252, R2: 0.832576\n",
      "Epoch 4, Fold 8: RMSE: 0.004846, R2: 0.845125\n",
      "Epoch 4, Fold 9: RMSE: 0.005091, R2: 0.830462\n",
      "Epoch 4, Fold 10: RMSE: 0.004277, R2: 0.869023\n",
      "Epoch 4 Average - RMSE: 0.005478, MSE: 0.000033, MAE: 0.002265, R2: 0.828545\n",
      "Epoch 5, Fold 1: RMSE: 0.005408, R2: 0.818375\n",
      "Epoch 5, Fold 2: RMSE: 0.009716, R2: 0.768977\n",
      "Epoch 5, Fold 3: RMSE: 0.004438, R2: 0.863711\n",
      "Epoch 5, Fold 4: RMSE: 0.004822, R2: 0.843767\n",
      "Epoch 5, Fold 5: RMSE: 0.005580, R2: 0.804592\n",
      "Epoch 5, Fold 6: RMSE: 0.004498, R2: 0.856804\n",
      "Epoch 5, Fold 7: RMSE: 0.005319, R2: 0.816298\n",
      "Epoch 5, Fold 8: RMSE: 0.004721, R2: 0.847971\n",
      "Epoch 5, Fold 9: RMSE: 0.005132, R2: 0.825923\n",
      "Epoch 5, Fold 10: RMSE: 0.004848, R2: 0.842505\n",
      "Epoch 5 Average - RMSE: 0.005448, MSE: 0.000032, MAE: 0.002263, R2: 0.828892\n",
      "Epoch 6, Fold 1: RMSE: 0.005294, R2: 0.819147\n",
      "Epoch 6, Fold 2: RMSE: 0.005499, R2: 0.810936\n",
      "Epoch 6, Fold 3: RMSE: 0.004554, R2: 0.859768\n",
      "Epoch 6, Fold 4: RMSE: 0.004938, R2: 0.841191\n",
      "Epoch 6, Fold 5: RMSE: 0.004481, R2: 0.861732\n",
      "Epoch 6, Fold 6: RMSE: 0.005523, R2: 0.810679\n",
      "Epoch 6, Fold 7: RMSE: 0.004479, R2: 0.857038\n",
      "Epoch 6, Fold 8: RMSE: 0.004353, R2: 0.863103\n",
      "Epoch 6, Fold 9: RMSE: 0.009468, R2: 0.776861\n",
      "Epoch 6, Fold 10: RMSE: 0.005583, R2: 0.802306\n",
      "Epoch 6 Average - RMSE: 0.005417, MSE: 0.000031, MAE: 0.002261, R2: 0.830276\n",
      "Epoch 7, Fold 1: RMSE: 0.009536, R2: 0.769166\n",
      "Epoch 7, Fold 2: RMSE: 0.005086, R2: 0.826236\n",
      "Epoch 7, Fold 3: RMSE: 0.004256, R2: 0.876745\n",
      "Epoch 7, Fold 4: RMSE: 0.004426, R2: 0.864484\n",
      "Epoch 7, Fold 5: RMSE: 0.004863, R2: 0.837645\n",
      "Epoch 7, Fold 6: RMSE: 0.004946, R2: 0.833931\n",
      "Epoch 7, Fold 7: RMSE: 0.005496, R2: 0.806014\n",
      "Epoch 7, Fold 8: RMSE: 0.004406, R2: 0.870093\n",
      "Epoch 7, Fold 9: RMSE: 0.004489, R2: 0.867514\n",
      "Epoch 7, Fold 10: RMSE: 0.006110, R2: 0.782908\n",
      "Epoch 7 Average - RMSE: 0.005361, MSE: 0.000031, MAE: 0.002262, R2: 0.833474\n",
      "Epoch 8, Fold 1: RMSE: 0.004162, R2: 0.879168\n",
      "Epoch 8, Fold 2: RMSE: 0.004162, R2: 0.876471\n",
      "Epoch 8, Fold 3: RMSE: 0.010060, R2: 0.753136\n",
      "Epoch 8, Fold 4: RMSE: 0.005232, R2: 0.818127\n",
      "Epoch 8, Fold 5: RMSE: 0.005117, R2: 0.832767\n",
      "Epoch 8, Fold 6: RMSE: 0.005101, R2: 0.832272\n",
      "Epoch 8, Fold 7: RMSE: 0.005363, R2: 0.814598\n",
      "Epoch 8, Fold 8: RMSE: 0.004595, R2: 0.853266\n",
      "Epoch 8, Fold 9: RMSE: 0.005447, R2: 0.804071\n",
      "Epoch 8, Fold 10: RMSE: 0.004714, R2: 0.860459\n",
      "Epoch 8 Average - RMSE: 0.005395, MSE: 0.000032, MAE: 0.002264, R2: 0.832433\n",
      "Epoch 9, Fold 1: RMSE: 0.004285, R2: 0.867674\n",
      "Epoch 9, Fold 2: RMSE: 0.004610, R2: 0.860953\n",
      "Epoch 9, Fold 3: RMSE: 0.008678, R2: 0.798303\n",
      "Epoch 9, Fold 4: RMSE: 0.005998, R2: 0.783748\n",
      "Epoch 9, Fold 5: RMSE: 0.005827, R2: 0.795971\n",
      "Epoch 9, Fold 6: RMSE: 0.004852, R2: 0.841140\n",
      "Epoch 9, Fold 7: RMSE: 0.004192, R2: 0.875472\n",
      "Epoch 9, Fold 8: RMSE: 0.005876, R2: 0.789018\n",
      "Epoch 9, Fold 9: RMSE: 0.004728, R2: 0.843531\n",
      "Epoch 9, Fold 10: RMSE: 0.004682, R2: 0.855249\n",
      "Epoch 9 Average - RMSE: 0.005373, MSE: 0.000030, MAE: 0.002261, R2: 0.831106\n",
      "Epoch 10, Fold 1: RMSE: 0.004298, R2: 0.867057\n",
      "Epoch 10, Fold 2: RMSE: 0.004911, R2: 0.841851\n",
      "Epoch 10, Fold 3: RMSE: 0.005476, R2: 0.812364\n",
      "Epoch 10, Fold 4: RMSE: 0.005145, R2: 0.825097\n",
      "Epoch 10, Fold 5: RMSE: 0.004988, R2: 0.832924\n",
      "Epoch 10, Fold 6: RMSE: 0.005128, R2: 0.832753\n",
      "Epoch 10, Fold 7: RMSE: 0.004854, R2: 0.844796\n",
      "Epoch 10, Fold 8: RMSE: 0.008782, R2: 0.793877\n",
      "Epoch 10, Fold 9: RMSE: 0.005729, R2: 0.796626\n",
      "Epoch 10, Fold 10: RMSE: 0.004900, R2: 0.839274\n",
      "Epoch 10 Average - RMSE: 0.005421, MSE: 0.000031, MAE: 0.002260, R2: 0.828662\n",
      "Epoch 11, Fold 1: RMSE: 0.004484, R2: 0.861831\n",
      "Epoch 11, Fold 2: RMSE: 0.010342, R2: 0.753243\n",
      "Epoch 11, Fold 3: RMSE: 0.005220, R2: 0.821064\n",
      "Epoch 11, Fold 4: RMSE: 0.004665, R2: 0.853658\n",
      "Epoch 11, Fold 5: RMSE: 0.004345, R2: 0.872864\n",
      "Epoch 11, Fold 6: RMSE: 0.005536, R2: 0.803276\n",
      "Epoch 11, Fold 7: RMSE: 0.005451, R2: 0.803558\n",
      "Epoch 11, Fold 8: RMSE: 0.004738, R2: 0.850697\n",
      "Epoch 11, Fold 9: RMSE: 0.005567, R2: 0.807105\n",
      "Epoch 11, Fold 10: RMSE: 0.004366, R2: 0.866110\n",
      "Epoch 11 Average - RMSE: 0.005471, MSE: 0.000033, MAE: 0.002270, R2: 0.829341\n",
      "Epoch 12, Fold 1: RMSE: 0.004317, R2: 0.869341\n",
      "Epoch 12, Fold 2: RMSE: 0.011148, R2: 0.704775\n",
      "Epoch 12, Fold 3: RMSE: 0.004742, R2: 0.849139\n",
      "Epoch 12, Fold 4: RMSE: 0.005718, R2: 0.783544\n",
      "Epoch 12, Fold 5: RMSE: 0.004040, R2: 0.882842\n",
      "Epoch 12, Fold 6: RMSE: 0.005677, R2: 0.796423\n",
      "Epoch 12, Fold 7: RMSE: 0.004775, R2: 0.848884\n",
      "Epoch 12, Fold 8: RMSE: 0.004720, R2: 0.847217\n",
      "Epoch 12, Fold 9: RMSE: 0.005124, R2: 0.826462\n",
      "Epoch 12, Fold 10: RMSE: 0.004939, R2: 0.849769\n",
      "Epoch 12 Average - RMSE: 0.005520, MSE: 0.000034, MAE: 0.002264, R2: 0.825840\n",
      "Epoch 13, Fold 1: RMSE: 0.004277, R2: 0.870791\n",
      "Epoch 13, Fold 2: RMSE: 0.004710, R2: 0.853181\n",
      "Epoch 13, Fold 3: RMSE: 0.008605, R2: 0.802189\n",
      "Epoch 13, Fold 4: RMSE: 0.004803, R2: 0.846907\n",
      "Epoch 13, Fold 5: RMSE: 0.005870, R2: 0.779671\n",
      "Epoch 13, Fold 6: RMSE: 0.004632, R2: 0.857905\n",
      "Epoch 13, Fold 7: RMSE: 0.004428, R2: 0.863552\n",
      "Epoch 13, Fold 8: RMSE: 0.006174, R2: 0.779436\n",
      "Epoch 13, Fold 9: RMSE: 0.005855, R2: 0.789701\n",
      "Epoch 13, Fold 10: RMSE: 0.004371, R2: 0.864343\n",
      "Epoch 13 Average - RMSE: 0.005373, MSE: 0.000030, MAE: 0.002262, R2: 0.830767\n",
      "Epoch 14, Fold 1: RMSE: 0.004521, R2: 0.856787\n",
      "Epoch 14, Fold 2: RMSE: 0.005533, R2: 0.807634\n",
      "Epoch 14, Fold 3: RMSE: 0.005247, R2: 0.820268\n",
      "Epoch 14, Fold 4: RMSE: 0.004731, R2: 0.852991\n",
      "Epoch 14, Fold 5: RMSE: 0.005082, R2: 0.833762\n",
      "Epoch 14, Fold 6: RMSE: 0.004490, R2: 0.855409\n",
      "Epoch 14, Fold 7: RMSE: 0.007066, R2: 0.719306\n",
      "Epoch 14, Fold 8: RMSE: 0.004436, R2: 0.864398\n",
      "Epoch 14, Fold 9: RMSE: 0.008469, R2: 0.804549\n",
      "Epoch 14, Fold 10: RMSE: 0.004538, R2: 0.862208\n",
      "Epoch 14 Average - RMSE: 0.005411, MSE: 0.000031, MAE: 0.002265, R2: 0.827731\n",
      "Epoch 15, Fold 1: RMSE: 0.004758, R2: 0.837888\n",
      "Epoch 15, Fold 2: RMSE: 0.005547, R2: 0.798151\n",
      "Epoch 15, Fold 3: RMSE: 0.005093, R2: 0.834374\n",
      "Epoch 15, Fold 4: RMSE: 0.005867, R2: 0.782235\n",
      "Epoch 15, Fold 5: RMSE: 0.005116, R2: 0.829734\n",
      "Epoch 15, Fold 6: RMSE: 0.005174, R2: 0.826608\n",
      "Epoch 15, Fold 7: RMSE: 0.004677, R2: 0.857619\n",
      "Epoch 15, Fold 8: RMSE: 0.005228, R2: 0.825582\n",
      "Epoch 15, Fold 9: RMSE: 0.009002, R2: 0.785126\n",
      "Epoch 15, Fold 10: RMSE: 0.004632, R2: 0.851687\n",
      "Epoch 15 Average - RMSE: 0.005510, MSE: 0.000032, MAE: 0.002264, R2: 0.822900\n",
      "Epoch 16, Fold 1: RMSE: 0.004098, R2: 0.880847\n",
      "Epoch 16, Fold 2: RMSE: 0.004998, R2: 0.840695\n",
      "Epoch 16, Fold 3: RMSE: 0.004965, R2: 0.835904\n",
      "Epoch 16, Fold 4: RMSE: 0.005104, R2: 0.830827\n",
      "Epoch 16, Fold 5: RMSE: 0.004320, R2: 0.868901\n",
      "Epoch 16, Fold 6: RMSE: 0.005169, R2: 0.818800\n",
      "Epoch 16, Fold 7: RMSE: 0.010310, R2: 0.743103\n",
      "Epoch 16, Fold 8: RMSE: 0.005731, R2: 0.799452\n",
      "Epoch 16, Fold 9: RMSE: 0.004517, R2: 0.861501\n",
      "Epoch 16, Fold 10: RMSE: 0.005409, R2: 0.806241\n",
      "Epoch 16 Average - RMSE: 0.005462, MSE: 0.000033, MAE: 0.002266, R2: 0.828627\n",
      "Epoch 17, Fold 1: RMSE: 0.004300, R2: 0.872822\n",
      "Epoch 17, Fold 2: RMSE: 0.004738, R2: 0.850059\n",
      "Epoch 17, Fold 3: RMSE: 0.005152, R2: 0.820397\n",
      "Epoch 17, Fold 4: RMSE: 0.004165, R2: 0.875170\n",
      "Epoch 17, Fold 5: RMSE: 0.010700, R2: 0.724045\n",
      "Epoch 17, Fold 6: RMSE: 0.005993, R2: 0.785231\n",
      "Epoch 17, Fold 7: RMSE: 0.005665, R2: 0.792161\n",
      "Epoch 17, Fold 8: RMSE: 0.004826, R2: 0.837808\n",
      "Epoch 17, Fold 9: RMSE: 0.005130, R2: 0.834692\n",
      "Epoch 17, Fold 10: RMSE: 0.004298, R2: 0.874390\n",
      "Epoch 17 Average - RMSE: 0.005497, MSE: 0.000034, MAE: 0.002262, R2: 0.826678\n",
      "Epoch 18, Fold 1: RMSE: 0.005077, R2: 0.831285\n",
      "Epoch 18, Fold 2: RMSE: 0.004733, R2: 0.847029\n",
      "Epoch 18, Fold 3: RMSE: 0.009163, R2: 0.780922\n",
      "Epoch 18, Fold 4: RMSE: 0.004358, R2: 0.872502\n",
      "Epoch 18, Fold 5: RMSE: 0.004525, R2: 0.862013\n",
      "Epoch 18, Fold 6: RMSE: 0.006289, R2: 0.762712\n",
      "Epoch 18, Fold 7: RMSE: 0.004541, R2: 0.855417\n",
      "Epoch 18, Fold 8: RMSE: 0.004745, R2: 0.847423\n",
      "Epoch 18, Fold 9: RMSE: 0.006011, R2: 0.771354\n",
      "Epoch 18, Fold 10: RMSE: 0.004730, R2: 0.856411\n",
      "Epoch 18 Average - RMSE: 0.005417, MSE: 0.000031, MAE: 0.002265, R2: 0.828707\n",
      "Epoch 19, Fold 1: RMSE: 0.004375, R2: 0.864987\n",
      "Epoch 19, Fold 2: RMSE: 0.009813, R2: 0.760710\n",
      "Epoch 19, Fold 3: RMSE: 0.004313, R2: 0.866597\n",
      "Epoch 19, Fold 4: RMSE: 0.005283, R2: 0.818993\n",
      "Epoch 19, Fold 5: RMSE: 0.005316, R2: 0.818076\n",
      "Epoch 19, Fold 6: RMSE: 0.005582, R2: 0.801050\n",
      "Epoch 19, Fold 7: RMSE: 0.004238, R2: 0.869649\n",
      "Epoch 19, Fold 8: RMSE: 0.004523, R2: 0.868973\n",
      "Epoch 19, Fold 9: RMSE: 0.005661, R2: 0.796689\n",
      "Epoch 19, Fold 10: RMSE: 0.005323, R2: 0.822771\n",
      "Epoch 19 Average - RMSE: 0.005443, MSE: 0.000032, MAE: 0.002261, R2: 0.828850\n",
      "Epoch 20, Fold 1: RMSE: 0.004426, R2: 0.870106\n",
      "Epoch 20, Fold 2: RMSE: 0.008727, R2: 0.801028\n",
      "Epoch 20, Fold 3: RMSE: 0.006207, R2: 0.767893\n",
      "Epoch 20, Fold 4: RMSE: 0.005016, R2: 0.844104\n",
      "Epoch 20, Fold 5: RMSE: 0.006474, R2: 0.752674\n",
      "Epoch 20, Fold 6: RMSE: 0.004251, R2: 0.869655\n",
      "Epoch 20, Fold 7: RMSE: 0.004788, R2: 0.844292\n",
      "Epoch 20, Fold 8: RMSE: 0.004537, R2: 0.857752\n",
      "Epoch 20, Fold 9: RMSE: 0.004792, R2: 0.841285\n",
      "Epoch 20, Fold 10: RMSE: 0.004470, R2: 0.861768\n",
      "Epoch 20 Average - RMSE: 0.005369, MSE: 0.000031, MAE: 0.002263, R2: 0.831056\n",
      "array([[5.51772816e-03, 3.18019140e-05, 2.26406636e-03, 8.22298762e-01],\n",
      "       [5.47246211e-03, 3.16435252e-05, 2.26347322e-03, 8.25293658e-01],\n",
      "       [5.42633044e-03, 3.09589183e-05, 2.26303674e-03, 8.27499539e-01],\n",
      "       [5.47838437e-03, 3.34253638e-05, 2.26527671e-03, 8.28544604e-01],\n",
      "       [5.44815642e-03, 3.18377287e-05, 2.26250858e-03, 8.28892337e-01],\n",
      "       [5.41726720e-03, 3.13804449e-05, 2.26089738e-03, 8.30276074e-01],\n",
      "       [5.36142315e-03, 3.09723107e-05, 2.26192778e-03, 8.33473548e-01],\n",
      "       [5.39535743e-03, 3.17204673e-05, 2.26419219e-03, 8.32433364e-01],\n",
      "       [5.37276882e-03, 3.04761502e-05, 2.26066619e-03, 8.31105892e-01],\n",
      "       [5.42103288e-03, 3.07733323e-05, 2.26016154e-03, 8.28661802e-01],\n",
      "       [5.47142653e-03, 3.27816046e-05, 2.27027689e-03, 8.29340602e-01],\n",
      "       [5.51990472e-03, 3.42359357e-05, 2.26424613e-03, 8.25839589e-01],\n",
      "       [5.37269198e-03, 3.04633400e-05, 2.26164040e-03, 8.30767461e-01],\n",
      "       [5.41119950e-03, 3.08865905e-05, 2.26459151e-03, 8.27731252e-01],\n",
      "       [5.50957351e-03, 3.18427564e-05, 2.26427646e-03, 8.22900265e-01],\n",
      "       [5.46210693e-03, 3.26661016e-05, 2.26600216e-03, 8.28627093e-01],\n",
      "       [5.49669556e-03, 3.35403170e-05, 2.26210183e-03, 8.26677569e-01],\n",
      "       [5.41723100e-03, 3.12800156e-05, 2.26502405e-03, 8.28706683e-01],\n",
      "       [5.44279221e-03, 3.20179538e-05, 2.26104312e-03, 8.28849537e-01],\n",
      "       [5.36886529e-03, 3.05879190e-05, 2.26345460e-03, 8.31055543e-01]])\n",
      "\n",
      "Mean over 20 runs (RMSE, MSE, MAE, R2): [5.43916991e-03 3.17646345e-05 2.26344319e-03 8.28448759e-01]\n",
      "Std  over 20 runs (RMSE, MSE, MAE, R2): [5.09046775e-05 1.07874605e-06 2.31498149e-06 2.85552109e-03]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  REAL OT-CATBOOST (WASSERSTEIN-1) FOR ENTRYPOINT ADDRESS v06 - FIXED\n",
    "# ================================================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d6/total 06.csv\"\n",
    "FEATS = ['Original_len', 'Txn Fee', 'logIndex', 'actualGasCost', 'Blockno', 'DateTime_ts', 'nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "# OT-CatBoost hyperparameters\n",
    "ROUNDS      = 5\n",
    "STAGE       = 600\n",
    "K_LAST      = 2\n",
    "LAM         = 0.07\n",
    "\n",
    "# ================== EPOCHS CONFIG ==================\n",
    "EPOCHS = 20\n",
    "\n",
    "# ===================================================\n",
    "# 1 DATA LOADING & PREPROCESSING\n",
    "# ===================================================\n",
    "use_cols = FEATS + [TARGET]\n",
    "df = pd.read_csv(PATH, usecols=lambda c: c in use_cols)\n",
    "\n",
    "for c in use_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "y_raw = df[TARGET].to_numpy(dtype=np.float64)\n",
    "\n",
    "# FIX: To train/test split  c th shuffle mi epoch\n",
    "X_full, y_full = X, y_raw\n",
    "\n",
    "# ===================================================\n",
    "# 2 HELPER FUNCTIONS\n",
    "# ===================================================\n",
    "def combine_last_k_leaf_ids(leaf_mat, k_last=1):\n",
    "    \"\"\"Combine last k leaf indexes into unique group IDs.\"\"\"\n",
    "    L = np.asarray(leaf_mat)\n",
    "    if L.ndim == 1:\n",
    "        return L.astype(np.int64)\n",
    "    k = max(1, min(int(k_last), L.shape[1]))\n",
    "    sub = L[:, -k:]\n",
    "    base = int(sub.max()) + 1\n",
    "    powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "    return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    \"\"\"Compute true Wasserstein-1 (Earth Mover's) distance.\"\"\"\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein-1 penalties per leaf group.\n",
    "    Penalize groups whose residual distributions deviate \n",
    "    significantly from the global residual distribution.\n",
    "    \"\"\"\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    n_leaves = len(unique_leaves)\n",
    "    penalties = np.zeros(n_leaves, dtype=np.float64)\n",
    "    global_residuals = residuals\n",
    "\n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        if len(leaf_residuals) < 8:\n",
    "            penalties[i] = 0.0\n",
    "            continue\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        penalties[i] = lam * w_dist\n",
    "    return penalties\n",
    "\n",
    "# ===================================================\n",
    "# 3 20 EPOCHS  10-FOLD CROSS VALIDATION - FIXED\n",
    "# ===================================================\n",
    "rd = np.zeros((EPOCHS, 4), dtype=np.float64)   # [RMSE, MSE, MAE, R2]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # FIX: To permutation cho mi epoch  c s bin i\n",
    "    idx = np.random.permutation(len(X_full))\n",
    "    X_ep = X_full[idx]\n",
    "    y_ep = y_full[idx]\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    metrics_all = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_ep), 1):\n",
    "        # Split t d liu  permuted\n",
    "        X_train, X_test = X_ep[train_idx], X_ep[test_idx]\n",
    "        y_train_raw, y_test_raw = y_ep[train_idx], y_ep[test_idx]\n",
    "\n",
    "        # Target scaling\n",
    "        y_mm_scaler = MinMaxScaler()\n",
    "        y_train_mm = y_mm_scaler.fit_transform(y_train_raw.reshape(-1, 1)).ravel()\n",
    "        y_test_mm  = y_mm_scaler.transform(y_test_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Feature impute + scale\n",
    "        X_train[~np.isfinite(X_train)] = np.nan\n",
    "        X_test[~np.isfinite(X_test)]   = np.nan\n",
    "        imp = SimpleImputer(strategy=\"median\").fit(X_train)\n",
    "        X_train_imp = imp.transform(X_train)\n",
    "        X_test_imp  = imp.transform(X_test)\n",
    "        scaler = MinMaxScaler().fit(X_train_imp)\n",
    "        X_train_s = scaler.transform(X_train_imp).astype(np.float32)\n",
    "        X_test_s  = scaler.transform(X_test_imp).astype(np.float32)\n",
    "\n",
    "        # Target transform (log1p)\n",
    "        y_train = np.log1p(y_train_raw.astype(np.float64))\n",
    "        y_test  = np.log1p(y_test_raw.astype(np.float64))\n",
    "\n",
    "        # ===================================================\n",
    "        # 4 TRAINING LOOP WITH REAL OT REWEIGHTING\n",
    "        # ===================================================\n",
    "        w = np.ones(len(y_train), dtype=np.float64)\n",
    "        model = None\n",
    "\n",
    "        for r in range(1, ROUNDS + 1):\n",
    "            iters = (model.tree_count_ if model is not None else 0) + STAGE\n",
    "            # FIX: Thm randomness cho CatBoost mi fold\n",
    "            booster = CatBoostRegressor(\n",
    "                iterations=iters, depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "                loss_function=\"RMSE\", \n",
    "                random_seed=SEED,  \n",
    "                verbose=False,\n",
    "                allow_writing_files=False, \n",
    "                thread_count=-1\n",
    "            )\n",
    "            booster.fit(\n",
    "                X_train_s, y_train, sample_weight=w,\n",
    "                init_model=None if model is None else model\n",
    "            )\n",
    "            model = booster\n",
    "\n",
    "            # Compute residuals\n",
    "            res = y_train - model.predict(X_train_s)\n",
    "\n",
    "            # Group residuals by leaf path\n",
    "            leaf_idx = model.calc_leaf_indexes(\n",
    "                X_train_s,\n",
    "                ntree_start=max(model.tree_count_ - K_LAST, 0),\n",
    "                ntree_end=model.tree_count_,\n",
    "                thread_count=-1\n",
    "            )\n",
    "            leaf_idx = np.asarray(leaf_idx)\n",
    "            if leaf_idx.ndim == 1:\n",
    "                leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "            codes = combine_last_k_leaf_ids(leaf_idx, k_last=K_LAST)\n",
    "            _, inv = np.unique(codes, return_inverse=True)\n",
    "\n",
    "            # Apply OT penalty (W1 distance)\n",
    "            pen = real_ot_penalty(residuals=res, leaf_assignments=inv, lam=LAM)\n",
    "\n",
    "            # Update weights\n",
    "            w *= np.exp(-pen[inv])\n",
    "            w /= (w.mean() + 1e-12)\n",
    "            w = np.clip(w, 1e-3, 50.0)\n",
    "\n",
    "        # ===================================================\n",
    "        # 5 EVALUATION ON TEST SET\n",
    "        # ===================================================\n",
    "        y_pred_raw = np.expm1(model.predict(X_test_s))\n",
    "        y_pred_mm  = y_mm_scaler.transform(y_pred_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "        mse  = mean_squared_error(y_test_mm, y_pred_mm)\n",
    "        rmse = mean_squared_error(y_test_mm, y_pred_mm, squared=False)\n",
    "        mae  = mean_absolute_error(y_test_mm, y_pred_mm)\n",
    "        r2   = r2_score(y_test_mm, y_pred_mm)\n",
    "\n",
    "        metrics_all.append([rmse, mse, mae, r2])\n",
    "        print(f\"Epoch {epoch+1}, Fold {fold}: RMSE: {rmse:.6f}, R2: {r2:.6f}\")\n",
    "\n",
    "    # Store epoch results\n",
    "    rd[epoch, :] = np.mean(metrics_all, axis=0)\n",
    "    print(f\"Epoch {epoch + 1} Average - RMSE: {rd[epoch, 0]:.6f}, MSE: {rd[epoch, 1]:.6f}, MAE: {rd[epoch, 2]:.6f}, R2: {rd[epoch, 3]:.6f}\")\n",
    "\n",
    "# ================== OUTPUT ==================\n",
    "np.set_printoptions(precision=8, suppress=False)\n",
    "print(\"array(\" + np.array2string(rd, separator=', ', prefix='array(') + \")\")\n",
    "\n",
    "final_mean = rd.mean(axis=0)\n",
    "final_std  = rd.std(axis=0, ddof=1)\n",
    "print(\"\\nMean over 20 runs (RMSE, MSE, MAE, R2):\", final_mean)\n",
    "print(\"Std  over 20 runs (RMSE, MSE, MAE, R2):\", final_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      " BEST PARAMETERS: {'stage': 600, 'rounds': 5, 'lam': 0.07, 'k_last': 2}\n",
      " Final R on held-out set: 0.917956\n"
     ]
    }
   ],
   "source": [
    "#optimize OT-Catboost, v07\n",
    "#optimize parameters OT-catboost with RandomizedSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ================== HELPER FUNCTIONS ==================\n",
    "def combine_last_k_leaf_ids(leaf_mat, k_last=1):\n",
    "    L = np.asarray(leaf_mat)\n",
    "    if L.ndim == 1:\n",
    "        return L.astype(np.int64)\n",
    "    k = max(1, min(int(k_last), L.shape[1]))\n",
    "    sub = L[:, -k:]\n",
    "    base = int(sub.max()) + 1\n",
    "    powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "    return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    penalties = np.zeros(len(unique_leaves), dtype=np.float64)\n",
    "    global_residuals = residuals\n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        if len(leaf_residuals) < 8:\n",
    "            continue\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        penalties[i] = lam * w_dist\n",
    "    return penalties\n",
    "\n",
    "# ================== CUSTOM ESTIMATOR ==================\n",
    "class OTCatBoostRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, lam=0.05, k_last=2, stage=400, rounds=5,\n",
    "                 depth=8, learning_rate=0.05, l2_leaf_reg=5):\n",
    "        self.lam = lam\n",
    "        self.k_last = k_last\n",
    "        self.stage = stage\n",
    "        self.rounds = rounds\n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_leaf_reg = l2_leaf_reg\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        w = np.ones(len(y), dtype=np.float64)\n",
    "        model = None\n",
    "        for r in range(1, self.rounds + 1):\n",
    "            iters = (model.tree_count_ if model else 0) + self.stage\n",
    "            booster = CatBoostRegressor(\n",
    "                iterations=iters,\n",
    "                depth=self.depth,\n",
    "                learning_rate=self.learning_rate,\n",
    "                l2_leaf_reg=self.l2_leaf_reg,\n",
    "                loss_function=\"RMSE\",\n",
    "                random_seed=SEED,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False,\n",
    "                thread_count=-1\n",
    "            )\n",
    "            booster.fit(X, y, sample_weight=w, init_model=None if model is None else model)\n",
    "            model = booster\n",
    "            res = y - model.predict(X)\n",
    "            leaf_idx = model.calc_leaf_indexes(X,\n",
    "                ntree_start=max(model.tree_count_ - self.k_last, 0),\n",
    "                ntree_end=model.tree_count_)\n",
    "            leaf_idx = np.asarray(leaf_idx)\n",
    "            if leaf_idx.ndim == 1:\n",
    "                leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "            codes = combine_last_k_leaf_ids(leaf_idx, self.k_last)\n",
    "            _, inv = np.unique(codes, return_inverse=True)\n",
    "            pen = real_ot_penalty(res, inv, self.lam)\n",
    "            w *= np.exp(-pen[inv])\n",
    "            w /= (w.mean() + 1e-12)\n",
    "            w = np.clip(w, 0.05, 10.0)\n",
    "        self.model_ = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "# ================== LOAD AND PREPROCESS ==================\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d7/total 7.csv\"\n",
    "FEATS = ['Original_len','Txn Fee','logIndex','actualGasCost','Blockno','DateTime_ts','nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "df = pd.read_csv(PATH, usecols=lambda c: c in FEATS + [TARGET])\n",
    "for c in FEATS + [TARGET]:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "df.dropna(subset=[TARGET], inplace=True)\n",
    "\n",
    "X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "y = df[TARGET].to_numpy(dtype=np.float64)\n",
    "\n",
    "X_train, X_val, y_train_raw, y_val_raw = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Impute + scale\n",
    "imp = SimpleImputer(strategy=\"median\").fit(X_train)\n",
    "X_train_imp = imp.transform(X_train)\n",
    "X_val_imp = imp.transform(X_val)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_imp)\n",
    "X_train_scaled = scaler.transform(X_train_imp).astype(np.float32)\n",
    "X_val_scaled = scaler.transform(X_val_imp).astype(np.float32)\n",
    "\n",
    "# Log transform\n",
    "y_train_log = np.log1p(y_train_raw.astype(np.float64))\n",
    "y_val_log = np.log1p(y_val_raw.astype(np.float64))\n",
    "\n",
    "# ================== RANDOM SEARCH CV ==================\n",
    "param_dist = {\n",
    "    'lam': [0.03, 0.05, 0.07],\n",
    "    'k_last': [2, 3, 4],\n",
    "    'stage': [400, 600, 800],\n",
    "    'rounds': [3, 5, 7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=OTCatBoostRegressor(),\n",
    "    param_distributions=param_dist,\n",
    "    scoring=make_scorer(r2_score),\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train_scaled, y_train_log)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# ================== EVALUATION ==================\n",
    "y_pred_log = best_model.predict(X_val_scaled)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_val_log)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\n BEST PARAMETERS:\", search.best_params_)\n",
    "print(f\" Final R on held-out set: {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "Fold 1 [MinMax]: RMSE=0.002486, MSE=0.000006, MAE=0.001086, R2=0.910448\n",
      "\n",
      "===== Fold 2 =====\n",
      "Fold 2 [MinMax]: RMSE=0.002342, MSE=0.000005, MAE=0.001078, R2=0.922410\n",
      "\n",
      "===== Fold 3 =====\n",
      "Fold 3 [MinMax]: RMSE=0.002265, MSE=0.000005, MAE=0.001067, R2=0.930850\n",
      "\n",
      "===== Fold 4 =====\n",
      "Fold 4 [MinMax]: RMSE=0.012077, MSE=0.000146, MAE=0.001260, R2=0.369809\n",
      "\n",
      "===== Fold 5 =====\n",
      "Fold 5 [MinMax]: RMSE=0.002373, MSE=0.000006, MAE=0.001053, R2=0.922563\n",
      "\n",
      "===== Fold 6 =====\n",
      "Fold 6 [MinMax]: RMSE=0.002523, MSE=0.000006, MAE=0.001088, R2=0.911719\n",
      "\n",
      "===== Fold 7 =====\n",
      "Fold 7 [MinMax]: RMSE=0.002767, MSE=0.000008, MAE=0.001106, R2=0.894107\n",
      "\n",
      "===== Fold 8 =====\n",
      "Fold 8 [MinMax]: RMSE=0.012302, MSE=0.000151, MAE=0.001198, R2=0.340577\n",
      "\n",
      "===== Fold 9 =====\n",
      "Fold 9 [MinMax]: RMSE=0.002556, MSE=0.000007, MAE=0.001070, R2=0.913413\n",
      "\n",
      "===== Fold 10 =====\n",
      "Fold 10 [MinMax]: RMSE=0.002268, MSE=0.000005, MAE=0.001063, R2=0.926022\n",
      "\n",
      "=== 10-FOLD AVERAGE (MinMax) ===\n",
      "RMSE: 0.004396\n",
      "MSE : 0.000035\n",
      "MAE : 0.001107\n",
      "R  : 0.804192\n"
     ]
    }
   ],
   "source": [
    "#optimze parameters of OT-Catboost\n",
    "#WASSSENTERIN 1/ optimized\n",
    "# ================================================================\n",
    "#  REAL OT-CATBOOST (WASSERSTEIN-1) FOR ENTRYPOINT ADDRESS v06\n",
    "# ================================================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d7/total 7.csv\"\n",
    "FEATS = ['Original_len', 'Txn Fee', 'logIndex', 'actualGasCost', 'Blockno', 'DateTime_ts', 'nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "# OT-CatBoost hyperparameters\n",
    "ROUNDS      = 5\n",
    "STAGE       = 600\n",
    "K_LAST      = 2\n",
    "LAM         = 0.07\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# 1 DATA LOADING & PREPROCESSING\n",
    "# ===================================================\n",
    "use_cols = FEATS + [TARGET]\n",
    "df = pd.read_csv(PATH, usecols=lambda c: c in use_cols)\n",
    "\n",
    "for c in use_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "y_raw = df[TARGET].to_numpy(dtype=np.float64)\n",
    "\n",
    "# ===================================================\n",
    "# 2 HELPER FUNCTIONS\n",
    "# ===================================================\n",
    "def combine_last_k_leaf_ids(leaf_mat, k_last=1):\n",
    "    \"\"\"Combine last k leaf indexes into unique group IDs.\"\"\"\n",
    "    L = np.asarray(leaf_mat)\n",
    "    if L.ndim == 1:\n",
    "        return L.astype(np.int64)\n",
    "    k = max(1, min(int(k_last), L.shape[1]))\n",
    "    sub = L[:, -k:]\n",
    "    base = int(sub.max()) + 1\n",
    "    powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "    return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    \"\"\"Compute true Wasserstein-1 (Earth Mover's) distance.\"\"\"\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein-1 penalties per leaf group.\n",
    "    Penalize groups whose residual distributions deviate \n",
    "    significantly from the global residual distribution.\n",
    "    \"\"\"\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    n_leaves = len(unique_leaves)\n",
    "    penalties = np.zeros(n_leaves, dtype=np.float64)\n",
    "    global_residuals = residuals\n",
    "\n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        if len(leaf_residuals) < 8:\n",
    "            penalties[i] = 0.0\n",
    "            continue\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        penalties[i] = lam * w_dist\n",
    "    return penalties\n",
    "\n",
    "# ===================================================\n",
    "# 3 10-FOLD CROSS VALIDATION\n",
    "# ===================================================\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "metrics_all = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train_raw, y_test_raw = y_raw[train_idx], y_raw[test_idx]\n",
    "\n",
    "    # Target scaling\n",
    "    y_mm_scaler = MinMaxScaler()\n",
    "    y_train_mm = y_mm_scaler.fit_transform(y_train_raw.reshape(-1, 1)).ravel()\n",
    "    y_test_mm  = y_mm_scaler.transform(y_test_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Feature impute + scale\n",
    "    X_train[~np.isfinite(X_train)] = np.nan\n",
    "    X_test[~np.isfinite(X_test)]   = np.nan\n",
    "    imp = SimpleImputer(strategy=\"median\").fit(X_train)\n",
    "    X_train_imp = imp.transform(X_train)\n",
    "    X_test_imp  = imp.transform(X_test)\n",
    "    scaler = MinMaxScaler().fit(X_train_imp)\n",
    "    X_train_s = scaler.transform(X_train_imp).astype(np.float32)\n",
    "    X_test_s  = scaler.transform(X_test_imp).astype(np.float32)\n",
    "\n",
    "    # Target transform (log1p)\n",
    "    y_train = np.log1p(y_train_raw.astype(np.float64))\n",
    "    y_test  = np.log1p(y_test_raw.astype(np.float64))\n",
    "\n",
    "    # ===================================================\n",
    "    # 4 TRAINING LOOP WITH REAL OT REWEIGHTING\n",
    "    # ===================================================\n",
    "    w = np.ones(len(y_train), dtype=np.float64)\n",
    "    model = None\n",
    "\n",
    "    for r in range(1, ROUNDS + 1):\n",
    "        iters = (model.tree_count_ if model is not None else 0) + STAGE\n",
    "        booster = CatBoostRegressor(\n",
    "            iterations=iters, depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "            loss_function=\"RMSE\", random_seed=SEED, verbose=False,\n",
    "            allow_writing_files=False, thread_count=-1\n",
    "        )\n",
    "        booster.fit(\n",
    "            X_train_s, y_train, sample_weight=w,\n",
    "            init_model=None if model is None else model\n",
    "        )\n",
    "        model = booster\n",
    "\n",
    "        # Compute residuals\n",
    "        res = y_train - model.predict(X_train_s)\n",
    "\n",
    "        # Group residuals by leaf path\n",
    "        leaf_idx = model.calc_leaf_indexes(\n",
    "            X_train_s,\n",
    "            ntree_start=max(model.tree_count_ - K_LAST, 0),\n",
    "            ntree_end=model.tree_count_,\n",
    "            thread_count=-1\n",
    "        )\n",
    "        leaf_idx = np.asarray(leaf_idx)\n",
    "        if leaf_idx.ndim == 1:\n",
    "            leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "        codes = combine_last_k_leaf_ids(leaf_idx, k_last=K_LAST)\n",
    "        _, inv = np.unique(codes, return_inverse=True)\n",
    "\n",
    "        # Apply OT penalty (W1 distance)\n",
    "        pen = real_ot_penalty(residuals=res, leaf_assignments=inv, lam=LAM)\n",
    "\n",
    "        # Update weights\n",
    "        w *= np.exp(-pen[inv])\n",
    "        w /= (w.mean() + 1e-12)\n",
    "        w = np.clip(w, 1e-3, 50.0)\n",
    "\n",
    "    # ===================================================\n",
    "    # 5 EVALUATION ON TEST SET\n",
    "    # ===================================================\n",
    "    y_pred_raw = np.expm1(model.predict(X_test_s))\n",
    "    y_pred_mm  = y_mm_scaler.transform(y_pred_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "    mse  = mean_squared_error(y_test_mm, y_pred_mm)\n",
    "    rmse = mean_squared_error(y_test_mm, y_pred_mm, squared=False)\n",
    "    mae  = mean_absolute_error(y_test_mm, y_pred_mm)\n",
    "    r2   = r2_score(y_test_mm, y_pred_mm)\n",
    "\n",
    "    print(f\"Fold {fold} [MinMax]: RMSE={rmse:.6f}, MSE={mse:.6f}, MAE={mae:.6f}, R2={r2:.6f}\")\n",
    "    metrics_all.append([rmse, mse, mae, r2])\n",
    "\n",
    "# ===================================================\n",
    "# 6 SUMMARY OF RESULTS\n",
    "# ===================================================\n",
    "metrics_all = np.array(metrics_all)\n",
    "print(\"\\n=== 10-FOLD AVERAGE (MinMax) ===\")\n",
    "print(f\"RMSE: {metrics_all[:,0].mean():.6f}\")\n",
    "print(f\"MSE : {metrics_all[:,1].mean():.6f}\")\n",
    "print(f\"MAE : {metrics_all[:,2].mean():.6f}\")\n",
    "print(f\"R  : {metrics_all[:,3].mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Fold 1: RMSE: 0.002393, R2: 0.922598\n",
      "Epoch 1, Fold 2: RMSE: 0.002736, R2: 0.900537\n",
      "Epoch 1, Fold 3: RMSE: 0.002442, R2: 0.914750\n",
      "Epoch 1, Fold 4: RMSE: 0.002348, R2: 0.923382\n",
      "Epoch 1, Fold 5: RMSE: 0.002432, R2: 0.917619\n",
      "Epoch 1, Fold 6: RMSE: 0.002357, R2: 0.921807\n",
      "Epoch 1, Fold 7: RMSE: 0.002638, R2: 0.909570\n",
      "Epoch 1, Fold 8: RMSE: 0.002727, R2: 0.897525\n",
      "Epoch 1, Fold 9: RMSE: 0.158173, R2: 0.188626\n",
      "Epoch 1, Fold 10: RMSE: 0.002235, R2: 0.927016\n",
      "Epoch 1 Average - RMSE: 0.018048, MSE: 0.002507, MAE: 0.002213, R2: 0.842343\n",
      "Epoch 2, Fold 1: RMSE: 0.002675, R2: 0.904233\n",
      "Epoch 2, Fold 2: RMSE: 0.002470, R2: 0.914081\n",
      "Epoch 2, Fold 3: RMSE: 0.002256, R2: 0.930054\n",
      "Epoch 2, Fold 4: RMSE: 0.002478, R2: 0.913913\n",
      "Epoch 2, Fold 5: RMSE: 0.012143, R2: 0.363649\n",
      "Epoch 2, Fold 6: RMSE: 0.002436, R2: 0.918022\n",
      "Epoch 2, Fold 7: RMSE: 0.002397, R2: 0.916491\n",
      "Epoch 2, Fold 8: RMSE: 0.002362, R2: 0.921412\n",
      "Epoch 2, Fold 9: RMSE: 0.012458, R2: 0.323768\n",
      "Epoch 2, Fold 10: RMSE: 0.002324, R2: 0.926846\n",
      "Epoch 2 Average - RMSE: 0.004400, MSE: 0.000035, MAE: 0.001118, R2: 0.803247\n",
      "Epoch 3, Fold 1: RMSE: 0.002422, R2: 0.921115\n",
      "Epoch 3, Fold 2: RMSE: 0.002380, R2: 0.920327\n",
      "Epoch 3, Fold 3: RMSE: 0.002447, R2: 0.918514\n",
      "Epoch 3, Fold 4: RMSE: 0.002426, R2: 0.915463\n",
      "Epoch 3, Fold 5: RMSE: 0.002431, R2: 0.918046\n",
      "Epoch 3, Fold 6: RMSE: 0.002511, R2: 0.913734\n",
      "Epoch 3, Fold 7: RMSE: 0.162146, R2: 0.191128\n",
      "Epoch 3, Fold 8: RMSE: 0.002357, R2: 0.924896\n",
      "Epoch 3, Fold 9: RMSE: 0.002202, R2: 0.928241\n",
      "Epoch 3, Fold 10: RMSE: 0.002734, R2: 0.898247\n",
      "Epoch 3 Average - RMSE: 0.018406, MSE: 0.002634, MAE: 0.002289, R2: 0.844971\n",
      "Epoch 4, Fold 1: RMSE: 0.002561, R2: 0.910116\n",
      "Epoch 4, Fold 2: RMSE: 0.012382, R2: 0.319587\n",
      "Epoch 4, Fold 3: RMSE: 0.003115, R2: 0.859312\n",
      "Epoch 4, Fold 4: RMSE: 0.002473, R2: 0.910458\n",
      "Epoch 4, Fold 5: RMSE: 0.012454, R2: 0.329531\n",
      "Epoch 4, Fold 6: RMSE: 0.002187, R2: 0.931392\n",
      "Epoch 4, Fold 7: RMSE: 0.002639, R2: 0.907903\n",
      "Epoch 4, Fold 8: RMSE: 0.002344, R2: 0.924361\n",
      "Epoch 4, Fold 9: RMSE: 0.002807, R2: 0.895153\n",
      "Epoch 4, Fold 10: RMSE: 0.002389, R2: 0.925878\n",
      "Epoch 4 Average - RMSE: 0.004535, MSE: 0.000036, MAE: 0.001115, R2: 0.791369\n",
      "Epoch 5, Fold 1: RMSE: 0.158312, R2: 0.186254\n",
      "Epoch 5, Fold 2: RMSE: 0.002311, R2: 0.924246\n",
      "Epoch 5, Fold 3: RMSE: 0.002456, R2: 0.921245\n",
      "Epoch 5, Fold 4: RMSE: 0.002518, R2: 0.910818\n",
      "Epoch 5, Fold 5: RMSE: 0.002632, R2: 0.898435\n",
      "Epoch 5, Fold 6: RMSE: 0.002381, R2: 0.920461\n",
      "Epoch 5, Fold 7: RMSE: 0.002407, R2: 0.921496\n",
      "Epoch 5, Fold 8: RMSE: 0.002521, R2: 0.916646\n",
      "Epoch 5, Fold 9: RMSE: 0.002461, R2: 0.917870\n",
      "Epoch 5, Fold 10: RMSE: 0.004054, R2: 0.768314\n",
      "Epoch 5 Average - RMSE: 0.018205, MSE: 0.002513, MAE: 0.002228, R2: 0.828578\n",
      "Epoch 6, Fold 1: RMSE: 0.002979, R2: 0.882846\n",
      "Epoch 6, Fold 2: RMSE: 0.002459, R2: 0.921404\n",
      "Epoch 6, Fold 3: RMSE: 0.002524, R2: 0.911423\n",
      "Epoch 6, Fold 4: RMSE: 0.002468, R2: 0.914610\n",
      "Epoch 6, Fold 5: RMSE: 0.002409, R2: 0.912725\n",
      "Epoch 6, Fold 6: RMSE: 0.002246, R2: 0.930002\n",
      "Epoch 6, Fold 7: RMSE: 0.002342, R2: 0.924035\n",
      "Epoch 6, Fold 8: RMSE: 0.011911, R2: 0.379152\n",
      "Epoch 6, Fold 9: RMSE: 0.012463, R2: 0.315658\n",
      "Epoch 6, Fold 10: RMSE: 0.002483, R2: 0.917738\n",
      "Epoch 6 Average - RMSE: 0.004428, MSE: 0.000035, MAE: 0.001109, R2: 0.800959\n",
      "Epoch 7, Fold 1: RMSE: 0.002396, R2: 0.921405\n",
      "Epoch 7, Fold 2: RMSE: 0.012484, R2: 0.327885\n",
      "Epoch 7, Fold 3: RMSE: 0.002264, R2: 0.925566\n",
      "Epoch 7, Fold 4: RMSE: 0.012107, R2: 0.360876\n",
      "Epoch 7, Fold 5: RMSE: 0.002377, R2: 0.922786\n",
      "Epoch 7, Fold 6: RMSE: 0.002209, R2: 0.933060\n",
      "Epoch 7, Fold 7: RMSE: 0.003022, R2: 0.876463\n",
      "Epoch 7, Fold 8: RMSE: 0.002448, R2: 0.918035\n",
      "Epoch 7, Fold 9: RMSE: 0.002482, R2: 0.913908\n",
      "Epoch 7, Fold 10: RMSE: 0.002315, R2: 0.922716\n",
      "Epoch 7 Average - RMSE: 0.004410, MSE: 0.000035, MAE: 0.001104, R2: 0.802270\n",
      "Epoch 8, Fold 1: RMSE: 0.002302, R2: 0.925182\n",
      "Epoch 8, Fold 2: RMSE: 0.002448, R2: 0.915961\n",
      "Epoch 8, Fold 3: RMSE: 0.002731, R2: 0.899233\n",
      "Epoch 8, Fold 4: RMSE: 0.002268, R2: 0.928471\n",
      "Epoch 8, Fold 5: RMSE: 0.002394, R2: 0.919415\n",
      "Epoch 8, Fold 6: RMSE: 0.002551, R2: 0.908386\n",
      "Epoch 8, Fold 7: RMSE: 0.002641, R2: 0.907050\n",
      "Epoch 8, Fold 8: RMSE: 0.002443, R2: 0.919004\n",
      "Epoch 8, Fold 9: RMSE: 0.002537, R2: 0.909705\n",
      "Epoch 8, Fold 10: RMSE: 0.157584, R2: 0.198344\n",
      "Epoch 8 Average - RMSE: 0.017990, MSE: 0.002489, MAE: 0.002249, R2: 0.843075\n",
      "Epoch 9, Fold 1: RMSE: 0.002271, R2: 0.930901\n",
      "Epoch 9, Fold 2: RMSE: 0.002704, R2: 0.897102\n",
      "Epoch 9, Fold 3: RMSE: 0.158296, R2: 0.191432\n",
      "Epoch 9, Fold 4: RMSE: 0.002645, R2: 0.905420\n",
      "Epoch 9, Fold 5: RMSE: 0.002333, R2: 0.926255\n",
      "Epoch 9, Fold 6: RMSE: 0.002270, R2: 0.927570\n",
      "Epoch 9, Fold 7: RMSE: 0.002356, R2: 0.919690\n",
      "Epoch 9, Fold 8: RMSE: 0.002464, R2: 0.917039\n",
      "Epoch 9, Fold 9: RMSE: 0.002828, R2: 0.894432\n",
      "Epoch 9, Fold 10: RMSE: 0.002217, R2: 0.927186\n",
      "Epoch 9 Average - RMSE: 0.018038, MSE: 0.002511, MAE: 0.002218, R2: 0.843703\n",
      "Epoch 10, Fold 1: RMSE: 0.002457, R2: 0.915723\n",
      "Epoch 10, Fold 2: RMSE: 0.002511, R2: 0.912389\n",
      "Epoch 10, Fold 3: RMSE: 0.002773, R2: 0.896802\n",
      "Epoch 10, Fold 4: RMSE: 0.002462, R2: 0.914982\n",
      "Epoch 10, Fold 5: RMSE: 0.012444, R2: 0.325446\n",
      "Epoch 10, Fold 6: RMSE: 0.002466, R2: 0.914583\n",
      "Epoch 10, Fold 7: RMSE: 0.011946, R2: 0.372671\n",
      "Epoch 10, Fold 8: RMSE: 0.002228, R2: 0.931019\n",
      "Epoch 10, Fold 9: RMSE: 0.002363, R2: 0.921959\n",
      "Epoch 10, Fold 10: RMSE: 0.002600, R2: 0.910995\n",
      "Epoch 10 Average - RMSE: 0.004425, MSE: 0.000035, MAE: 0.001112, R2: 0.801657\n",
      "Epoch 11, Fold 1: RMSE: 0.002682, R2: 0.899456\n",
      "Epoch 11, Fold 2: RMSE: 0.002417, R2: 0.915820\n",
      "Epoch 11, Fold 3: RMSE: 0.002476, R2: 0.914976\n",
      "Epoch 11, Fold 4: RMSE: 0.002278, R2: 0.923947\n",
      "Epoch 11, Fold 5: RMSE: 0.157971, R2: 0.196007\n",
      "Epoch 11, Fold 6: RMSE: 0.002196, R2: 0.935353\n",
      "Epoch 11, Fold 7: RMSE: 0.002613, R2: 0.909588\n",
      "Epoch 11, Fold 8: RMSE: 0.002636, R2: 0.904125\n",
      "Epoch 11, Fold 9: RMSE: 0.002419, R2: 0.922196\n",
      "Epoch 11, Fold 10: RMSE: 0.003104, R2: 0.864558\n",
      "Epoch 11 Average - RMSE: 0.018079, MSE: 0.002501, MAE: 0.002224, R2: 0.838603\n",
      "Epoch 12, Fold 1: RMSE: 0.002752, R2: 0.895573\n",
      "Epoch 12, Fold 2: RMSE: 0.002392, R2: 0.921912\n",
      "Epoch 12, Fold 3: RMSE: 0.002800, R2: 0.895267\n",
      "Epoch 12, Fold 4: RMSE: 0.002857, R2: 0.888362\n",
      "Epoch 12, Fold 5: RMSE: 0.012375, R2: 0.320571\n",
      "Epoch 12, Fold 6: RMSE: 0.002607, R2: 0.908430\n",
      "Epoch 12, Fold 7: RMSE: 0.002282, R2: 0.927615\n",
      "Epoch 12, Fold 8: RMSE: 0.011927, R2: 0.371863\n",
      "Epoch 12, Fold 9: RMSE: 0.002229, R2: 0.930591\n",
      "Epoch 12, Fold 10: RMSE: 0.002401, R2: 0.921672\n",
      "Epoch 12 Average - RMSE: 0.004462, MSE: 0.000035, MAE: 0.001113, R2: 0.798186\n",
      "Epoch 13, Fold 1: RMSE: 0.012350, R2: 0.314476\n",
      "Epoch 13, Fold 2: RMSE: 0.002533, R2: 0.914372\n",
      "Epoch 13, Fold 3: RMSE: 0.002833, R2: 0.893446\n",
      "Epoch 13, Fold 4: RMSE: 0.002761, R2: 0.897212\n",
      "Epoch 13, Fold 5: RMSE: 0.002423, R2: 0.917726\n",
      "Epoch 13, Fold 6: RMSE: 0.002352, R2: 0.922787\n",
      "Epoch 13, Fold 7: RMSE: 0.002612, R2: 0.906556\n",
      "Epoch 13, Fold 8: RMSE: 0.012187, R2: 0.350072\n",
      "Epoch 13, Fold 9: RMSE: 0.002456, R2: 0.914456\n",
      "Epoch 13, Fold 10: RMSE: 0.002459, R2: 0.919411\n",
      "Epoch 13 Average - RMSE: 0.004497, MSE: 0.000035, MAE: 0.001111, R2: 0.795051\n",
      "Epoch 14, Fold 1: RMSE: 0.002429, R2: 0.918652\n",
      "Epoch 14, Fold 2: RMSE: 0.002255, R2: 0.927922\n",
      "Epoch 14, Fold 3: RMSE: 0.002592, R2: 0.911699\n",
      "Epoch 14, Fold 4: RMSE: 0.002407, R2: 0.920022\n",
      "Epoch 14, Fold 5: RMSE: 0.002691, R2: 0.902063\n",
      "Epoch 14, Fold 6: RMSE: 0.002833, R2: 0.882540\n",
      "Epoch 14, Fold 7: RMSE: 0.012519, R2: 0.325919\n",
      "Epoch 14, Fold 8: RMSE: 0.012049, R2: 0.366189\n",
      "Epoch 14, Fold 9: RMSE: 0.002772, R2: 0.896057\n",
      "Epoch 14, Fold 10: RMSE: 0.002244, R2: 0.925593\n",
      "Epoch 14 Average - RMSE: 0.004479, MSE: 0.000035, MAE: 0.001111, R2: 0.797666\n",
      "Epoch 15, Fold 1: RMSE: 0.002370, R2: 0.922126\n",
      "Epoch 15, Fold 2: RMSE: 0.002622, R2: 0.906138\n",
      "Epoch 15, Fold 3: RMSE: 0.012218, R2: 0.339698\n",
      "Epoch 15, Fold 4: RMSE: 0.002297, R2: 0.922846\n",
      "Epoch 15, Fold 5: RMSE: 0.012399, R2: 0.334514\n",
      "Epoch 15, Fold 6: RMSE: 0.002292, R2: 0.921378\n",
      "Epoch 15, Fold 7: RMSE: 0.002395, R2: 0.926699\n",
      "Epoch 15, Fold 8: RMSE: 0.002588, R2: 0.911960\n",
      "Epoch 15, Fold 9: RMSE: 0.002664, R2: 0.904686\n",
      "Epoch 15, Fold 10: RMSE: 0.002489, R2: 0.911915\n",
      "Epoch 15 Average - RMSE: 0.004434, MSE: 0.000035, MAE: 0.001110, R2: 0.800196\n",
      "Epoch 16, Fold 1: RMSE: 0.002549, R2: 0.912520\n",
      "Epoch 16, Fold 2: RMSE: 0.002474, R2: 0.913767\n",
      "Epoch 16, Fold 3: RMSE: 0.002732, R2: 0.901046\n",
      "Epoch 16, Fold 4: RMSE: 0.002433, R2: 0.918746\n",
      "Epoch 16, Fold 5: RMSE: 0.011980, R2: 0.366014\n",
      "Epoch 16, Fold 6: RMSE: 0.002521, R2: 0.914124\n",
      "Epoch 16, Fold 7: RMSE: 0.002264, R2: 0.926396\n",
      "Epoch 16, Fold 8: RMSE: 0.012490, R2: 0.315942\n",
      "Epoch 16, Fold 9: RMSE: 0.002611, R2: 0.906560\n",
      "Epoch 16, Fold 10: RMSE: 0.002416, R2: 0.919386\n",
      "Epoch 16 Average - RMSE: 0.004447, MSE: 0.000035, MAE: 0.001110, R2: 0.799450\n",
      "Epoch 17, Fold 1: RMSE: 0.012420, R2: 0.321354\n",
      "Epoch 17, Fold 2: RMSE: 0.002331, R2: 0.924451\n",
      "Epoch 17, Fold 3: RMSE: 0.002851, R2: 0.890959\n",
      "Epoch 17, Fold 4: RMSE: 0.002503, R2: 0.919633\n",
      "Epoch 17, Fold 5: RMSE: 0.002407, R2: 0.919028\n",
      "Epoch 17, Fold 6: RMSE: 0.012330, R2: 0.328194\n",
      "Epoch 17, Fold 7: RMSE: 0.002840, R2: 0.890940\n",
      "Epoch 17, Fold 8: RMSE: 0.002507, R2: 0.913100\n",
      "Epoch 17, Fold 9: RMSE: 0.002208, R2: 0.929169\n",
      "Epoch 17, Fold 10: RMSE: 0.002364, R2: 0.922674\n",
      "Epoch 17 Average - RMSE: 0.004476, MSE: 0.000036, MAE: 0.001111, R2: 0.795950\n",
      "Epoch 18, Fold 1: RMSE: 0.002351, R2: 0.920247\n",
      "Epoch 18, Fold 2: RMSE: 0.012076, R2: 0.354170\n",
      "Epoch 18, Fold 3: RMSE: 0.002492, R2: 0.915051\n",
      "Epoch 18, Fold 4: RMSE: 0.002651, R2: 0.906105\n",
      "Epoch 18, Fold 5: RMSE: 0.012352, R2: 0.334663\n",
      "Epoch 18, Fold 6: RMSE: 0.002490, R2: 0.913458\n",
      "Epoch 18, Fold 7: RMSE: 0.002797, R2: 0.896246\n",
      "Epoch 18, Fold 8: RMSE: 0.002331, R2: 0.925170\n",
      "Epoch 18, Fold 9: RMSE: 0.002366, R2: 0.922477\n",
      "Epoch 18, Fold 10: RMSE: 0.002329, R2: 0.925483\n",
      "Epoch 18 Average - RMSE: 0.004423, MSE: 0.000035, MAE: 0.001110, R2: 0.801307\n",
      "Epoch 19, Fold 1: RMSE: 0.002458, R2: 0.918963\n",
      "Epoch 19, Fold 2: RMSE: 0.012388, R2: 0.328868\n",
      "Epoch 19, Fold 3: RMSE: 0.002504, R2: 0.917144\n",
      "Epoch 19, Fold 4: RMSE: 0.011832, R2: 0.391103\n",
      "Epoch 19, Fold 5: RMSE: 0.003444, R2: 0.837064\n",
      "Epoch 19, Fold 6: RMSE: 0.002362, R2: 0.923834\n",
      "Epoch 19, Fold 7: RMSE: 0.002391, R2: 0.918620\n",
      "Epoch 19, Fold 8: RMSE: 0.002526, R2: 0.909868\n",
      "Epoch 19, Fold 9: RMSE: 0.002435, R2: 0.918610\n",
      "Epoch 19, Fold 10: RMSE: 0.002439, R2: 0.912927\n",
      "Epoch 19 Average - RMSE: 0.004478, MSE: 0.000035, MAE: 0.001114, R2: 0.797700\n",
      "Epoch 20, Fold 1: RMSE: 0.002339, R2: 0.922073\n",
      "Epoch 20, Fold 2: RMSE: 0.012255, R2: 0.349965\n",
      "Epoch 20, Fold 3: RMSE: 0.002415, R2: 0.920924\n",
      "Epoch 20, Fold 4: RMSE: 0.002600, R2: 0.904905\n",
      "Epoch 20, Fold 5: RMSE: 0.002580, R2: 0.908799\n",
      "Epoch 20, Fold 6: RMSE: 0.002371, R2: 0.919786\n",
      "Epoch 20, Fold 7: RMSE: 0.002723, R2: 0.901097\n",
      "Epoch 20, Fold 8: RMSE: 0.012498, R2: 0.319590\n",
      "Epoch 20, Fold 9: RMSE: 0.002337, R2: 0.921785\n",
      "Epoch 20, Fold 10: RMSE: 0.002266, R2: 0.930246\n",
      "Epoch 20 Average - RMSE: 0.004438, MSE: 0.000035, MAE: 0.001112, R2: 0.799917\n",
      "array([[1.80480930e-02, 2.50743168e-03, 2.21299145e-03, 8.42343018e-01],\n",
      "       [4.39983522e-03, 3.49792512e-05, 1.11796330e-03, 8.03246763e-01],\n",
      "       [1.84056352e-02, 2.63447582e-03, 2.28853690e-03, 8.44971174e-01],\n",
      "       [4.53505350e-03, 3.61619034e-05, 1.11457063e-03, 7.91369147e-01],\n",
      "       [1.82052814e-02, 2.51277738e-03, 2.22780499e-03, 8.28578497e-01],\n",
      "       [4.42830658e-03, 3.47067288e-05, 1.10913201e-03, 8.00959312e-01],\n",
      "       [4.41025549e-03, 3.50449732e-05, 1.10351616e-03, 8.02269906e-01],\n",
      "       [1.79899036e-02, 2.48882278e-03, 2.24882533e-03, 8.43074890e-01],\n",
      "       [1.80383055e-02, 2.51122774e-03, 2.21816474e-03, 8.43702835e-01],\n",
      "       [4.42490109e-03, 3.47030993e-05, 1.11231764e-03, 8.01657031e-01],\n",
      "       [1.80792470e-02, 2.50132207e-03, 2.22352390e-03, 8.38602633e-01],\n",
      "       [4.46235587e-03, 3.47437520e-05, 1.11334139e-03, 7.98185521e-01],\n",
      "       [4.49670327e-03, 3.53433012e-05, 1.11130691e-03, 7.95051376e-01],\n",
      "       [4.47891349e-03, 3.53365163e-05, 1.11090923e-03, 7.97665553e-01],\n",
      "       [4.43368401e-03, 3.51791885e-05, 1.10998325e-03, 8.00195870e-01],\n",
      "       [4.44713686e-03, 3.49664632e-05, 1.10967070e-03, 7.99450197e-01],\n",
      "       [4.47624991e-03, 3.56741440e-05, 1.11146391e-03, 7.95950422e-01],\n",
      "       [4.42348779e-03, 3.47651402e-05, 1.11010053e-03, 8.01306983e-01],\n",
      "       [4.47779451e-03, 3.47182522e-05, 1.11402110e-03, 7.97700075e-01],\n",
      "       [4.43827686e-03, 3.54743370e-05, 1.11229435e-03, 7.99917089e-01]])\n",
      "\n",
      "Mean over 20 runs (RMSE, MSE, MAE, R2): [8.55497100e-03 7.82392726e-04 1.44902192e-03 8.11309915e-01]\n",
      "Std  over 20 runs (RMSE, MSE, MAE, R2): [0.00643021 0.00117144 0.00052922 0.01984169]\n"
     ]
    }
   ],
   "source": [
    "#optimized /20 times/ \n",
    "#optimizzed, v06, 20 tun times, v07\n",
    "# ================================================================\n",
    "#  REAL OT-CATBOOST (WASSERSTEIN-1) FOR ENTRYPOINT ADDRESS v06 - FIXED\n",
    "# ================================================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d7/total 7.csv\"\n",
    "FEATS = ['Original_len', 'Txn Fee', 'logIndex', 'actualGasCost', 'Blockno', 'DateTime_ts', 'nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "# OT-CatBoost hyperparameters\n",
    "ROUNDS      = 5\n",
    "STAGE       = 600\n",
    "K_LAST      = 2\n",
    "LAM         = 0.07\n",
    "\n",
    "# ================== EPOCHS CONFIG ==================\n",
    "EPOCHS = 20\n",
    "\n",
    "# ===================================================\n",
    "# 1 DATA LOADING & PREPROCESSING\n",
    "# ===================================================\n",
    "use_cols = FEATS + [TARGET]\n",
    "df = pd.read_csv(PATH, usecols=lambda c: c in use_cols)\n",
    "\n",
    "for c in use_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "y_raw = df[TARGET].to_numpy(dtype=np.float64)\n",
    "\n",
    "# FIX: To train/test split  c th shuffle mi epoch\n",
    "X_full, y_full = X, y_raw\n",
    "\n",
    "# ===================================================\n",
    "# 2 HELPER FUNCTIONS\n",
    "# ===================================================\n",
    "def combine_last_k_leaf_ids(leaf_mat, k_last=1):\n",
    "    \"\"\"Combine last k leaf indexes into unique group IDs.\"\"\"\n",
    "    L = np.asarray(leaf_mat)\n",
    "    if L.ndim == 1:\n",
    "        return L.astype(np.int64)\n",
    "    k = max(1, min(int(k_last), L.shape[1]))\n",
    "    sub = L[:, -k:]\n",
    "    base = int(sub.max()) + 1\n",
    "    powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "    return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    \"\"\"Compute true Wasserstein-1 (Earth Mover's) distance.\"\"\"\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein-1 penalties per leaf group.\n",
    "    Penalize groups whose residual distributions deviate \n",
    "    significantly from the global residual distribution.\n",
    "    \"\"\"\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    n_leaves = len(unique_leaves)\n",
    "    penalties = np.zeros(n_leaves, dtype=np.float64)\n",
    "    global_residuals = residuals\n",
    "\n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        if len(leaf_residuals) < 8:\n",
    "            penalties[i] = 0.0\n",
    "            continue\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        penalties[i] = lam * w_dist\n",
    "    return penalties\n",
    "\n",
    "# ===================================================\n",
    "# 3 20 EPOCHS  10-FOLD CROSS VALIDATION - FIXED\n",
    "# ===================================================\n",
    "rd = np.zeros((EPOCHS, 4), dtype=np.float64)   # [RMSE, MSE, MAE, R2]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # FIX: To permutation cho mi epoch  c s bin i\n",
    "    idx = np.random.permutation(len(X_full))\n",
    "    X_ep = X_full[idx]\n",
    "    y_ep = y_full[idx]\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    metrics_all = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_ep), 1):\n",
    "        # Split t d liu  permuted\n",
    "        X_train, X_test = X_ep[train_idx], X_ep[test_idx]\n",
    "        y_train_raw, y_test_raw = y_ep[train_idx], y_ep[test_idx]\n",
    "\n",
    "        # Target scaling\n",
    "        y_mm_scaler = MinMaxScaler()\n",
    "        y_train_mm = y_mm_scaler.fit_transform(y_train_raw.reshape(-1, 1)).ravel()\n",
    "        y_test_mm  = y_mm_scaler.transform(y_test_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Feature impute + scale\n",
    "        X_train[~np.isfinite(X_train)] = np.nan\n",
    "        X_test[~np.isfinite(X_test)]   = np.nan\n",
    "        imp = SimpleImputer(strategy=\"median\").fit(X_train)\n",
    "        X_train_imp = imp.transform(X_train)\n",
    "        X_test_imp  = imp.transform(X_test)\n",
    "        scaler = MinMaxScaler().fit(X_train_imp)\n",
    "        X_train_s = scaler.transform(X_train_imp).astype(np.float32)\n",
    "        X_test_s  = scaler.transform(X_test_imp).astype(np.float32)\n",
    "\n",
    "        # Target transform (log1p)\n",
    "        y_train = np.log1p(y_train_raw.astype(np.float64))\n",
    "        y_test  = np.log1p(y_test_raw.astype(np.float64))\n",
    "\n",
    "        # ===================================================\n",
    "        # 4 TRAINING LOOP WITH REAL OT REWEIGHTING\n",
    "        # ===================================================\n",
    "        w = np.ones(len(y_train), dtype=np.float64)\n",
    "        model = None\n",
    "\n",
    "        for r in range(1, ROUNDS + 1):\n",
    "            iters = (model.tree_count_ if model is not None else 0) + STAGE\n",
    "            # FIX: Thm randomness cho CatBoost mi fold\n",
    "            booster = CatBoostRegressor(\n",
    "                iterations=iters, depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "                loss_function=\"RMSE\", \n",
    "                random_seed=SEED,  \n",
    "                verbose=False,\n",
    "                allow_writing_files=False, \n",
    "                thread_count=-1\n",
    "            )\n",
    "            booster.fit(\n",
    "                X_train_s, y_train, sample_weight=w,\n",
    "                init_model=None if model is None else model\n",
    "            )\n",
    "            model = booster\n",
    "\n",
    "            # Compute residuals\n",
    "            res = y_train - model.predict(X_train_s)\n",
    "\n",
    "            # Group residuals by leaf path\n",
    "            leaf_idx = model.calc_leaf_indexes(\n",
    "                X_train_s,\n",
    "                ntree_start=max(model.tree_count_ - K_LAST, 0),\n",
    "                ntree_end=model.tree_count_,\n",
    "                thread_count=-1\n",
    "            )\n",
    "            leaf_idx = np.asarray(leaf_idx)\n",
    "            if leaf_idx.ndim == 1:\n",
    "                leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "            codes = combine_last_k_leaf_ids(leaf_idx, k_last=K_LAST)\n",
    "            _, inv = np.unique(codes, return_inverse=True)\n",
    "\n",
    "            # Apply OT penalty (W1 distance)\n",
    "            pen = real_ot_penalty(residuals=res, leaf_assignments=inv, lam=LAM)\n",
    "\n",
    "            # Update weights\n",
    "            w *= np.exp(-pen[inv])\n",
    "            w /= (w.mean() + 1e-12)\n",
    "            w = np.clip(w, 1e-3, 50.0)\n",
    "\n",
    "        # ===================================================\n",
    "        # 5 EVALUATION ON TEST SET\n",
    "        # ===================================================\n",
    "        y_pred_raw = np.expm1(model.predict(X_test_s))\n",
    "        y_pred_mm  = y_mm_scaler.transform(y_pred_raw.reshape(-1, 1)).ravel()\n",
    "\n",
    "        mse  = mean_squared_error(y_test_mm, y_pred_mm)\n",
    "        rmse = mean_squared_error(y_test_mm, y_pred_mm, squared=False)\n",
    "        mae  = mean_absolute_error(y_test_mm, y_pred_mm)\n",
    "        r2   = r2_score(y_test_mm, y_pred_mm)\n",
    "\n",
    "        metrics_all.append([rmse, mse, mae, r2])\n",
    "        print(f\"Epoch {epoch+1}, Fold {fold}: RMSE: {rmse:.6f}, R2: {r2:.6f}\")\n",
    "\n",
    "    # Store epoch results\n",
    "    rd[epoch, :] = np.mean(metrics_all, axis=0)\n",
    "    print(f\"Epoch {epoch + 1} Average - RMSE: {rd[epoch, 0]:.6f}, MSE: {rd[epoch, 1]:.6f}, MAE: {rd[epoch, 2]:.6f}, R2: {rd[epoch, 3]:.6f}\")\n",
    "\n",
    "# ================== OUTPUT ==================\n",
    "np.set_printoptions(precision=8, suppress=False)\n",
    "print(\"array(\" + np.array2string(rd, separator=', ', prefix='array(') + \")\")\n",
    "\n",
    "final_mean = rd.mean(axis=0)\n",
    "final_std  = rd.std(axis=0, ddof=1)\n",
    "print(\"\\nMean over 20 runs (RMSE, MSE, MAE, R2):\", final_mean)\n",
    "print(\"Std  over 20 runs (RMSE, MSE, MAE, R2):\", final_std)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      " Best CatBoost hyperparameters:\n",
      "   depth: 10\n",
      "   l2_leaf_reg: 3.855787045127016\n",
      "   learning_rate: 0.020431573091335004\n"
     ]
    }
   ],
   "source": [
    "#prove contribution of Catboost, OT, reweight,\n",
    "# ================== IMPORTS ==================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy import stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "SEED = 42; np.random.seed(SEED)\n",
    "PATH = \"C:/Users/Multiplexon/Desktop/data/d7/total 7 - Copy.csv\"\n",
    "FEATS = ['Original_len','Txn Fee','logIndex','actualGasCost','Blockno','DateTime_ts','nonce']\n",
    "TARGET = 'Gas Used'\n",
    "\n",
    "# Real OT Parameters\n",
    "ROUNDS = 5\n",
    "STAGE = 600\n",
    "LAM = 0.08  # Optimized based on previous experiments\n",
    "K_LAST = 3\n",
    "N_Q = 128\n",
    "\n",
    "# Experimental Setup\n",
    "N_FOLDS = 5\n",
    "N_BOOTSTRAPS = 1000  # For confidence intervals\n",
    "\n",
    "# ================== REAL OT IMPLEMENTATION ==================\n",
    "def real_wasserstein_distance(residuals_leaf, residuals_global, n_samples=1000):\n",
    "    \"\"\"Calculate real Wasserstein distance between distributions\"\"\"\n",
    "    if len(residuals_leaf) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Sample for efficiency\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if len(residuals_leaf) > n_samples:\n",
    "        residuals_leaf = rng.choice(residuals_leaf, n_samples, replace=False)\n",
    "    if len(residuals_global) > n_samples:\n",
    "        residuals_global = rng.choice(residuals_global, n_samples, replace=False)\n",
    "    \n",
    "    return wasserstein_distance(residuals_leaf, residuals_global)\n",
    "\n",
    "def real_ot_penalty(residuals, leaf_assignments, lam=0.1):\n",
    "    \"\"\"Real OT penalty calculation using Wasserstein distance\"\"\"\n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    n_leaves = len(unique_leaves)\n",
    "    penalties = np.zeros(n_leaves, dtype=np.float64)\n",
    "    \n",
    "    global_residuals = residuals\n",
    "    \n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        leaf_residuals = residuals[leaf_mask]\n",
    "        \n",
    "        if len(leaf_residuals) < 8:  # Minimum samples\n",
    "            penalties[i] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # Calculate real Wasserstein distance\n",
    "        w_dist = real_wasserstein_distance(leaf_residuals, global_residuals)\n",
    "        \n",
    "        # Additional statistical tests for problem detection\n",
    "        leaf_mean = np.mean(leaf_residuals)\n",
    "        leaf_std = np.std(leaf_residuals)\n",
    "        global_std = np.std(global_residuals)\n",
    "        \n",
    "        # Combined penalty based on multiple criteria\n",
    "        base_penalty = w_dist * lam\n",
    "        \n",
    "        # Boost penalty for clear problematic leaves\n",
    "        if abs(leaf_mean) > 0.25:  # High bias\n",
    "            base_penalty *= 1.5\n",
    "        if leaf_std > global_std * 2.0:  # High variance\n",
    "            base_penalty *= 1.3\n",
    "            \n",
    "        penalties[i] = base_penalty\n",
    "    \n",
    "    return penalties\n",
    "\n",
    "def feature_aware_ot_penalty(residuals, leaf_assignments, features, lam=0.1):\n",
    "    \"\"\"Enhanced OT with feature distribution awareness\"\"\"\n",
    "    base_penalties = real_ot_penalty(residuals, leaf_assignments, lam)\n",
    "    feature_penalties = np.zeros_like(base_penalties)\n",
    "    \n",
    "    unique_leaves = np.unique(leaf_assignments)\n",
    "    global_features = features\n",
    "    \n",
    "    for i, leaf_id in enumerate(unique_leaves):\n",
    "        leaf_mask = (leaf_assignments == leaf_id)\n",
    "        if np.sum(leaf_mask) < 10:\n",
    "            continue\n",
    "            \n",
    "        leaf_features = features[leaf_mask]\n",
    "        feature_mismatch_score = 0\n",
    "        \n",
    "        # Check important features for distribution mismatches\n",
    "        important_features = [0, 1, 4]  # Gas-related features\n",
    "        \n",
    "        for feat_idx in important_features:\n",
    "            global_feat = global_features[:, feat_idx]\n",
    "            leaf_feat = leaf_features[:, feat_idx]\n",
    "            \n",
    "            if len(np.unique(leaf_feat)) > 5:\n",
    "                # KS test for feature distribution difference\n",
    "                ks_stat, _ = stats.ks_2samp(global_feat, leaf_feat)\n",
    "                if ks_stat > 0.15:  # Significant difference\n",
    "                    feature_mismatch_score += ks_stat\n",
    "        \n",
    "        if feature_mismatch_score > 0.2:\n",
    "            feature_penalties[i] = feature_mismatch_score * 0.5\n",
    "    \n",
    "    return base_penalties + feature_penalties\n",
    "\n",
    "# ================== COMPREHENSIVE TRAINING ==================\n",
    "def train_real_ot_catboost(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train with real OT implementation\"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    w = np.ones(n_samples, dtype=np.float64)\n",
    "    model = None\n",
    "    ot_metrics_history = []\n",
    "    \n",
    "    for r in range(1, ROUNDS + 1):\n",
    "        iters = (model.tree_count_ if model is not None else 0) + STAGE\n",
    "        \n",
    "        booster = CatBoostRegressor(\n",
    "            iterations=iters, depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "            loss_function=\"RMSE\", random_seed=SEED, verbose=False,\n",
    "            allow_writing_files=False, thread_count=-1\n",
    "        )\n",
    "        booster.fit(X_train, y_train, sample_weight=w, eval_set=(X_val, y_val))\n",
    "        model = booster\n",
    "\n",
    "        # Real OT weight updates\n",
    "        res = y_train - model.predict(X_train)\n",
    "        leaf_idx = model.calc_leaf_indexes(\n",
    "            X_train,\n",
    "            ntree_start=max(model.tree_count_ - K_LAST, 0),\n",
    "            ntree_end=model.tree_count_,\n",
    "            thread_count=-1\n",
    "        )\n",
    "        leaf_idx = np.asarray(leaf_idx)\n",
    "        if leaf_idx.ndim == 1:\n",
    "            leaf_idx = leaf_idx.reshape(-1, 1)\n",
    "        \n",
    "        # Combine leaf assignments\n",
    "        def combine_leaves(leaf_mat, k_last=K_LAST):\n",
    "            L = np.asarray(leaf_mat)\n",
    "            if L.ndim == 1:\n",
    "                return L.astype(np.int64)\n",
    "            k = max(1, min(int(k_last), L.shape[1]))\n",
    "            sub = L[:, -k:]\n",
    "            base = int(sub.max()) + 1\n",
    "            powers = (base ** np.arange(sub.shape[1], dtype=np.int64))\n",
    "            return (sub.astype(np.int64) * powers).sum(axis=1)\n",
    "        \n",
    "        codes = combine_leaves(leaf_idx, k_last=K_LAST)\n",
    "        _, inv = np.unique(codes, return_inverse=True)\n",
    "        leaf_assignments = inv\n",
    "\n",
    "        # Apply real OT penalties\n",
    "        pen = feature_aware_ot_penalty(res, leaf_assignments, X_train, lam=LAM)\n",
    "        \n",
    "        # Track OT metrics\n",
    "        ot_metrics = {\n",
    "            'round': r,\n",
    "            'mean_penalty': np.mean(pen[pen > 0]),\n",
    "            'leaves_penalized': np.sum(pen > 0.01),\n",
    "            'max_penalty': np.max(pen) if len(pen) > 0 else 0,\n",
    "            'mean_w_distance': np.mean([real_wasserstein_distance(\n",
    "                res[leaf_assignments == i], res) \n",
    "                for i in np.unique(leaf_assignments) \n",
    "                if np.sum(leaf_assignments == i) >= 8])\n",
    "        }\n",
    "        ot_metrics_history.append(ot_metrics)\n",
    "        \n",
    "        # Update weights with real exponential penalties\n",
    "        w *= np.exp(-pen[leaf_assignments])\n",
    "        w /= (np.mean(w) + 1e-12)\n",
    "        w = np.clip(w, 0.05, 10.0)  # Reasonable constraints\n",
    "\n",
    "    # Final prediction\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    return metrics, model, ot_metrics_history\n",
    "\n",
    "def train_residual_baseline(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Residual weighting baseline for comparison\"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    w = np.ones(n_samples, dtype=np.float64)\n",
    "    model = None\n",
    "    \n",
    "    for r in range(1, ROUNDS + 1):\n",
    "        iters = (model.tree_count_ if model is not None else 0) + STAGE\n",
    "        \n",
    "        booster = CatBoostRegressor(\n",
    "            iterations=iters, depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "            loss_function=\"RMSE\", random_seed=SEED, verbose=False,\n",
    "            allow_writing_files=False, thread_count=-1\n",
    "        )\n",
    "        booster.fit(X_train, y_train, sample_weight=w, eval_set=(X_val, y_val))\n",
    "        model = booster\n",
    "\n",
    "        # Standard residual weighting\n",
    "        res = y_train - model.predict(X_train)\n",
    "        residual_abs = np.abs(res)\n",
    "        epsilon = 1e-6\n",
    "        w_new = 1.0 / (residual_abs + epsilon)\n",
    "        w_new = w_new / np.mean(w_new)\n",
    "        w = np.clip(w_new, 1e-3, 50.0)\n",
    "    \n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "    return calculate_metrics(y_test, y_pred), model\n",
    "\n",
    "def train_standard_baseline(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Standard CatBoost baseline\"\"\"\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=ROUNDS * STAGE,\n",
    "        depth=8, learning_rate=0.05, l2_leaf_reg=5,\n",
    "        loss_function=\"RMSE\", random_seed=SEED, verbose=False,\n",
    "        allow_writing_files=False, thread_count=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "    \n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "    return calculate_metrics(y_test, y_pred), model\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n",
    "    \n",
    "    return {'rmse': rmse, 'mse': mse, 'mae': mae, 'r2': r2, 'mape': mape}\n",
    "\n",
    "# ================== STATISTICAL EXPERIMENT ==================\n",
    "def run_statistical_experiment():\n",
    "    \"\"\"Run comprehensive experiment with statistical testing\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    use_cols = FEATS + [TARGET]\n",
    "    df = pd.read_csv(PATH, usecols=lambda c: c in use_cols)\n",
    "    for c in use_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.loc[df[TARGET] < -1, TARGET] = np.nan\n",
    "    df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "    X = df[FEATS].to_numpy(dtype=np.float64)\n",
    "    y_raw = df[TARGET].to_numpy(dtype=np.float64)\n",
    "    \n",
    "    print(\" Running Real OT Value Proof Experiment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    methods = {\n",
    "        'Standard_CatBoost': train_standard_baseline,\n",
    "        'Residual_Weighting': train_residual_baseline,\n",
    "        'Real_OT_CatBoost': train_real_ot_catboost\n",
    "    }\n",
    "    \n",
    "    results = {method: [] for method in methods.keys()}\n",
    "    detailed_results = {method: {'r2_scores': [], 'rmse_scores': []} for method in methods.keys()}\n",
    "    \n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\n Fold {fold}/{N_FOLDS}\")\n",
    "        \n",
    "        # Train/test split\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train_raw, y_test_raw = y_raw[train_idx], y_raw[test_idx]\n",
    "        \n",
    "        # Additional validation split\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train, y_train_raw, test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        \n",
    "        # Preprocessing\n",
    "        X_tr = np.nan_to_num(X_tr, nan=np.nanmedian(X_tr, axis=0))\n",
    "        X_val = np.nan_to_num(X_val, nan=np.nanmedian(X_tr, axis=0))\n",
    "        X_test = np.nan_to_num(X_test, nan=np.nanmedian(X_tr, axis=0))\n",
    "        \n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr).astype(np.float32)\n",
    "        X_val_s = scaler.transform(X_val).astype(np.float32)\n",
    "        X_test_s = scaler.transform(X_test).astype(np.float32)\n",
    "        \n",
    "        # Log transform targets for training\n",
    "        y_tr_log = np.log1p(y_tr.astype(np.float64))\n",
    "        y_val_log = np.log1p(y_val.astype(np.float64))\n",
    "        \n",
    "        # Test all methods\n",
    "        for method_name, method_func in methods.items():\n",
    "            try:\n",
    "                if 'OT' in method_name:\n",
    "                    metrics, model, ot_history = method_func(\n",
    "                        X_tr_s, y_tr_log, X_val_s, y_val_log, X_test_s, y_test_raw\n",
    "                    )\n",
    "                    print(f\"  {method_name}: R = {metrics['r2']:.6f} | OT Leaves Penalized: {ot_history[-1]['leaves_penalized'] if ot_history else 'N/A'}\")\n",
    "                else:\n",
    "                    metrics, model = method_func(\n",
    "                        X_tr_s, y_tr_log, X_val_s, y_val_log, X_test_s, y_test_raw\n",
    "                    )\n",
    "                    print(f\"  {method_name}: R = {metrics['r2']:.6f}\")\n",
    "                \n",
    "                results[method_name].append(metrics)\n",
    "                detailed_results[method_name]['r2_scores'].append(metrics['r2'])\n",
    "                detailed_results[method_name]['rmse_scores'].append(metrics['rmse'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {method_name} failed: {e}\")\n",
    "                # Append NaN to maintain alignment\n",
    "                results[method_name].append({k: np.nan for k in ['rmse', 'mse', 'mae', 'r2', 'mape']})\n",
    "                detailed_results[method_name]['r2_scores'].append(np.nan)\n",
    "                detailed_results[method_name]['rmse_scores'].append(np.nan)\n",
    "    \n",
    "    return results, detailed_results\n",
    "\n",
    "# ================== STATISTICAL ANALYSIS ==================\n",
    "def perform_statistical_analysis(detailed_results):\n",
    "    \"\"\"Perform rigorous statistical analysis\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Clean data\n",
    "    clean_results = {}\n",
    "    for method in detailed_results:\n",
    "        r2_scores = [x for x in detailed_results[method]['r2_scores'] if not np.isnan(x)]\n",
    "        rmse_scores = [x for x in detailed_results[method]['rmse_scores'] if not np.isnan(x)]\n",
    "        if len(r2_scores) >= 3:  # Minimum for statistical tests\n",
    "            clean_results[method] = {\n",
    "                'r2_scores': r2_scores,\n",
    "                'rmse_scores': rmse_scores\n",
    "            }\n",
    "    \n",
    "    methods = list(clean_results.keys())\n",
    "    if len(methods) < 2:\n",
    "        print(\" Not enough valid results for statistical analysis\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n PERFORMANCE SUMMARY (Mean  Std):\")\n",
    "    for method in methods:\n",
    "        r2_mean = np.mean(clean_results[method]['r2_scores'])\n",
    "        r2_std = np.std(clean_results[method]['r2_scores'])\n",
    "        rmse_mean = np.mean(clean_results[method]['rmse_scores'])\n",
    "        rmse_std = np.std(clean_results[method]['rmse_scores'])\n",
    "        print(f\"  {method:<20}: R = {r2_mean:.6f}  {r2_std:.6f} | RMSE = {rmse_mean:.2f}  {rmse_std:.2f}\")\n",
    "    \n",
    "    # Statistical tests\n",
    "    if 'Real_OT_CatBoost' in clean_results and 'Residual_Weighting' in clean_results:\n",
    "        ot_r2 = clean_results['Real_OT_CatBoost']['r2_scores']\n",
    "        res_r2 = clean_results['Residual_Weighting']['r2_scores']\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat_r2, p_value_r2 = stats.ttest_rel(ot_r2, res_r2)\n",
    "        \n",
    "        # Wilcoxon signed-rank test (non-parametric)\n",
    "        w_stat, p_value_wilcoxon = stats.wilcoxon(ot_r2, res_r2)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        diff = np.array(ot_r2) - np.array(res_r2)\n",
    "        cohens_d = np.mean(diff) / (np.std(diff, ddof=1) + 1e-12)\n",
    "        \n",
    "        print(f\"\\n REAL OT vs RESIDUAL WEIGHTING:\")\n",
    "        print(f\"  Paired t-test:     t = {t_stat_r2:.4f}, p = {p_value_r2:.6f}\")\n",
    "        print(f\"  Wilcoxon test:     W = {w_stat:.4f}, p = {p_value_wilcoxon:.6f}\")\n",
    "        print(f\"  Effect size:       Cohen's d = {cohens_d:.4f}\")\n",
    "        print(f\"  Mean improvement:  {np.mean(diff):+.6f} R\")\n",
    "        \n",
    "        # Bootstrap confidence intervals\n",
    "        bootstrap_diffs = []\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        for _ in range(N_BOOTSTRAPS):\n",
    "            sample_idx = rng.integers(0, len(diff), size=len(diff))\n",
    "            bootstrap_diffs.append(np.mean(diff[sample_idx]))\n",
    "        \n",
    "        ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
    "        ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
    "        print(f\"  95% CI:           [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "        \n",
    "        # Significance conclusion\n",
    "        alpha = 0.05\n",
    "        mean_improvement = np.mean(diff)\n",
    "        if p_value_r2 < alpha and mean_improvement > 0:\n",
    "            improvement_pct = (mean_improvement / np.mean(res_r2)) * 100\n",
    "            print(f\"   CONCLUSION:    REAL OT significantly outperforms Residual Weighting\")\n",
    "            print(f\"                   (+{improvement_pct:.2f}% improvement, p < {alpha})\")\n",
    "        else:\n",
    "            print(f\"   CONCLUSION:    No statistically significant improvement\")\n",
    "    \n",
    "    return clean_results\n",
    "\n",
    "def calculate_practical_significance(clean_results):\n",
    "    \"\"\"Calculate practical significance metrics\"\"\"\n",
    "    \n",
    "    print(f\"\\n PRACTICAL SIGNIFICANCE ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if 'Real_OT_CatBoost' in clean_results and 'Residual_Weighting' in clean_results:\n",
    "        ot_r2 = np.mean(clean_results['Real_OT_CatBoost']['r2_scores'])\n",
    "        res_r2 = np.mean(clean_results['Residual_Weighting']['r2_scores'])\n",
    "        std_r2 = np.mean(clean_results['Residual_Weighting']['r2_scores'])\n",
    "        \n",
    "        improvement = ot_r2 - res_r2\n",
    "        improvement_pct = (improvement / res_r2) * 100\n",
    "        \n",
    "        # Practical significance thresholds\n",
    "        thresholds = {\n",
    "            'negligible': 0.001,\n",
    "            'small': 0.003, \n",
    "            'moderate': 0.006,\n",
    "            'large': 0.01\n",
    "        }\n",
    "        \n",
    "        print(f\"  Absolute improvement: {improvement:.6f} R\")\n",
    "        print(f\"  Relative improvement: {improvement_pct:+.2f}%\")\n",
    "        \n",
    "        if improvement > thresholds['large']:\n",
    "            print(f\"   PRACTICAL IMPACT: LARGE (meaningful business value)\")\n",
    "        elif improvement > thresholds['moderate']:\n",
    "            print(f\"   PRACTICAL IMPACT: MODERATE (potentially valuable)\")\n",
    "        elif improvement > thresholds['small']:\n",
    "            print(f\"   PRACTICAL IMPACT: SMALL (marginal but measurable)\")\n",
    "        elif improvement > thresholds['negligible']:\n",
    "            print(f\"   PRACTICAL IMPACT: NEGLIGIBLE (not meaningful)\")\n",
    "        else:\n",
    "            print(f\"   PRACTICAL IMPACT: NONE (below noise level)\")\n",
    "        \n",
    "        # Cost-benefit analysis\n",
    "        computation_cost_ratio = 1.2  # OT is ~20% more expensive\n",
    "        benefit_cost_ratio = improvement / 0.001  # Normalized\n",
    "        \n",
    "        if benefit_cost_ratio > 2.0:\n",
    "            print(f\"   COST-BENEFIT: FAVORABLE (benefits > costs)\")\n",
    "        elif benefit_cost_ratio > 1.0:\n",
    "            print(f\"   COST-BENEFIT: MARGINAL (benefits  costs)\")  \n",
    "        else:\n",
    "            print(f\"   COST-BENEFIT: UNFAVORABLE (costs > benefits)\")\n",
    "\n",
    "# ================== MAIN EXECUTION ==================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" PROVING REAL OT VALUE - COMPREHENSIVE EXPERIMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"This experiment will:\")\n",
    "    print(\"1.  Use REAL OT implementation (Wasserstein distance)\")\n",
    "    print(\"2.  Compare against proper baselines\") \n",
    "    print(\"3.  Perform rigorous statistical testing\")\n",
    "    print(\"4.  Assess practical significance\")\n",
    "    print(\"5.  Provide clear conclusion about OT value\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run experiment\n",
    "    results, detailed_results = run_statistical_experiment()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    clean_results = perform_statistical_analysis(detailed_results)\n",
    "    \n",
    "    # Practical significance\n",
    "    if clean_results:\n",
    "        calculate_practical_significance(clean_results)\n",
    "    \n",
    "    # Final verdict\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" FINAL VERDICT: REAL OT VALUE ASSESSMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if clean_results and 'Real_OT_CatBoost' in clean_results:\n",
    "        ot_mean = np.mean(clean_results['Real_OT_CatBoost']['r2_scores'])\n",
    "        res_mean = np.mean(clean_results['Residual_Weighting']['r2_scores'])\n",
    "        improvement = ot_mean - res_mean\n",
    "        \n",
    "        if improvement > 0.005 and len([x for x in clean_results['Real_OT_CatBoost']['r2_scores'] if x > res_mean]) >= 3:\n",
    "            print(\" REAL OT PROVIDES MEANINGFUL VALUE\")\n",
    "            print(f\"    Statistical significance: Likely\")\n",
    "            print(f\"    Practical impact: Measurable\") \n",
    "            print(f\"    Recommendation: USE in production\")\n",
    "        elif improvement > 0.002:\n",
    "            print(\"  REAL OT PROVIDES MARGINAL VALUE\")\n",
    "            print(f\"    Statistical significance: Possible\")\n",
    "            print(f\"    Practical impact: Small\")\n",
    "            print(f\"    Recommendation: Consider for specific use cases\")\n",
    "        else:\n",
    "            print(\" REAL OT DOES NOT PROVIDE MEANINGFUL VALUE\")\n",
    "            print(f\"    Statistical significance: Unlikely\")\n",
    "            print(f\"    Practical impact: Negligible\")\n",
    "            print(f\"    Recommendation: STICK with Residual Weighting\")\n",
    "    else:\n",
    "        print(\" INCONCLUSIVE: Not enough data for final verdict\")\n",
    "    \n",
    "    print(f\"\\n Key Insight: Real OT value depends on:\")\n",
    "    print(\"   - Dataset characteristics (distribution mismatches)\")\n",
    "    print(\"   - Implementation quality (proper Wasserstein distance)\")\n",
    "    print(\"   - Parameter tuning (LAM, K_LAST, etc.)\")\n",
    "    print(\"   - Business context (required improvement threshold)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
